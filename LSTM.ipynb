{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 16:55:48.585149: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-16 16:55:48.630724: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-16 16:55:48.630762: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-16 16:55:48.630786: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-16 16:55:48.639166: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-16 16:55:49.515665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"myData2.parquet\"\n",
    "df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(20)\n",
    "# df['state'].value_counts()\n",
    "# df.info(max_cols=100)\n",
    "# data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['timestamp_seconds', \n",
    "                     'node_memory_Percpu_bytes', \n",
    "                     'node_context_switches_total', \n",
    "                     'surfsara_power_usage', \n",
    "                     'node_netstat_Tcp_InSegs', \n",
    "                     'node_netstat_Tcp_OutSegs', \n",
    "                     'node_network_transmit_packets_total-sum', \n",
    "                     'node_filesystem_size_bytes-sum', \n",
    "                     'node_filesystem_files-sum', \n",
    "                     'node_memory_MemFree_bytes', \n",
    "                     'node_netstat_Tcp_InErrs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_selected = df[['timestamp', 'state'] + selected_features].copy()\n",
    "\n",
    "# Encode the target variable 'state' to binary (0 for \"COMPLETED\", 1 otherwise)\n",
    "df_selected['target'] = (df_selected['state'] != 'COMPLETED').astype(int)\n",
    "\n",
    "# Drop the original 'state' column\n",
    "df_selected.drop('state', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time intervals\n",
    "time_intervals = {'minute': '1T', 'hour': '1H', 'day': '1D'}\n",
    "\n",
    "# Normalize selected features\n",
    "scaler = MinMaxScaler()\n",
    "df_selected[selected_features] = scaler.fit_transform(df_selected[selected_features])\n",
    "\n",
    "# Set sequence length\n",
    "sequence_length = 30\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps = 7\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for LSTM\n",
    "def prepare_lstm_data(data, time_interval):\n",
    "    data.set_index('timestamp', inplace=True)\n",
    "    data_resampled = data.resample(time_interval).sum()\n",
    "    data_resampled['target'] = data_resampled['target'].clip(upper=1)  # Clip values to 1\n",
    "    return data_resampled\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_lstm_sequences(data, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length].values\n",
    "        target = data.iloc[i+sequence_length]['target']\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new data\n",
    "def predict_future_failures(model, input_data, sequence_length, prediction_steps):\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(prediction_steps):\n",
    "        # Make a prediction for the next time step\n",
    "        prediction = model.predict(input_data.reshape(1, sequence_length, input_data.shape[1]))\n",
    "        predictions.append(prediction[0, 0])\n",
    "\n",
    "        # Shift the input data by one time step and append the new prediction\n",
    "        input_data = np.roll(input_data, shift=-1, axis=0)\n",
    "        input_data[-1, -1] = prediction[0, 0]\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 days -> 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 2s 494ms/step - loss: 0.1999 - mean_squared_error: 0.1999 - val_loss: 0.1533 - val_mean_squared_error: 0.1533\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1991 - mean_squared_error: 0.1991 - val_loss: 0.1524 - val_mean_squared_error: 0.1524\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1982 - mean_squared_error: 0.1982 - val_loss: 0.1518 - val_mean_squared_error: 0.1518\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1976 - mean_squared_error: 0.1976 - val_loss: 0.1511 - val_mean_squared_error: 0.1511\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.1970 - mean_squared_error: 0.1970 - val_loss: 0.1504 - val_mean_squared_error: 0.1504\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1964 - mean_squared_error: 0.1964 - val_loss: 0.1497 - val_mean_squared_error: 0.1497\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1957 - mean_squared_error: 0.1957 - val_loss: 0.1491 - val_mean_squared_error: 0.1491\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1950 - mean_squared_error: 0.1950 - val_loss: 0.1484 - val_mean_squared_error: 0.1484\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.1944 - mean_squared_error: 0.1944 - val_loss: 0.1478 - val_mean_squared_error: 0.1478\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.1938 - mean_squared_error: 0.1938 - val_loss: 0.1471 - val_mean_squared_error: 0.1471\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.1932 - mean_squared_error: 0.1932 - val_loss: 0.1464 - val_mean_squared_error: 0.1464\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1926 - mean_squared_error: 0.1926 - val_loss: 0.1458 - val_mean_squared_error: 0.1458\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.1920 - mean_squared_error: 0.1920 - val_loss: 0.1452 - val_mean_squared_error: 0.1452\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1914 - mean_squared_error: 0.1914 - val_loss: 0.1446 - val_mean_squared_error: 0.1446\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1910 - mean_squared_error: 0.1910 - val_loss: 0.1440 - val_mean_squared_error: 0.1440\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.1904 - mean_squared_error: 0.1904 - val_loss: 0.1435 - val_mean_squared_error: 0.1435\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.1898 - mean_squared_error: 0.1898 - val_loss: 0.1429 - val_mean_squared_error: 0.1429\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.1894 - mean_squared_error: 0.1894 - val_loss: 0.1424 - val_mean_squared_error: 0.1424\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.1888 - mean_squared_error: 0.1888 - val_loss: 0.1419 - val_mean_squared_error: 0.1419\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1883 - mean_squared_error: 0.1883 - val_loss: 0.1414 - val_mean_squared_error: 0.1414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd847deded0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for LSTM with daily intervals\n",
    "lstm_data_day = prepare_lstm_data(df_selected, time_intervals['day'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_day, targets_day = create_lstm_sequences(lstm_data_day, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_day, X_test_day, y_train_day, y_test_day = train_test_split(sequences_day, targets_day, test_size=0.3, random_state=1)\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model_day = Sequential()\n",
    "lstm_model_day.add(LSTM(50, input_shape=(X_train_day.shape[1], X_train_day.shape[2])))\n",
    "lstm_model_day.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_day.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model_day.fit(X_train_day, y_train_day, epochs=20, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 0.1502\n",
      "\n",
      "1/1 [==============================] - 0s 374ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted failures for the next 7 days:\n",
      "[0.736503  0.6537022 0.705062  0.6537022 0.6537022 0.6537022 0.6537022]\n",
      "Mean Absolute Error for Predictions: 0.3271\n",
      "Mean Squared Error for Predictions: 0.1080\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squared Error\n",
    "mse_day = lstm_model_day.evaluate(X_test_day, y_test_day, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mse_day:.4f}\\n')\n",
    "\n",
    "input_data = X_test_day[3]  # Can be any valid starting point\n",
    "\n",
    "# Make predictions\n",
    "predicted_failures = predict_future_failures(lstm_model_day, input_data, sequence_length, prediction_steps)\n",
    "\n",
    "# Denormalize the predicted failures \n",
    "predicted_failures_denormalized = predicted_failures * (lstm_data_day['target'].max() - lstm_data_day['target'].min()) + lstm_data_day['target'].min()\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted failures for the next 7 days:\")\n",
    "print(predicted_failures_denormalized)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures - y_test_day[3:3+prediction_steps])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures - y_test_day[3:3+prediction_steps]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 hours -> 7 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "54/54 [==============================] - 4s 27ms/step - loss: 0.2555 - mean_squared_error: 0.2555 - val_loss: 0.2372 - val_mean_squared_error: 0.2372\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2169 - mean_squared_error: 0.2169 - val_loss: 0.2179 - val_mean_squared_error: 0.2179\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2076 - mean_squared_error: 0.2076 - val_loss: 0.2117 - val_mean_squared_error: 0.2117\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2038 - mean_squared_error: 0.2038 - val_loss: 0.2087 - val_mean_squared_error: 0.2087\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.2013 - mean_squared_error: 0.2013 - val_loss: 0.2072 - val_mean_squared_error: 0.2072\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.1998 - mean_squared_error: 0.1998 - val_loss: 0.2059 - val_mean_squared_error: 0.2059\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.1986 - mean_squared_error: 0.1986 - val_loss: 0.2052 - val_mean_squared_error: 0.2052\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.1976 - mean_squared_error: 0.1976 - val_loss: 0.2048 - val_mean_squared_error: 0.2048\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.1966 - mean_squared_error: 0.1966 - val_loss: 0.2043 - val_mean_squared_error: 0.2043\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.1958 - mean_squared_error: 0.1958 - val_loss: 0.2041 - val_mean_squared_error: 0.2041\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.1952 - mean_squared_error: 0.1952 - val_loss: 0.2039 - val_mean_squared_error: 0.2039\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.1946 - mean_squared_error: 0.1946 - val_loss: 0.2037 - val_mean_squared_error: 0.2037\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.1941 - mean_squared_error: 0.1941 - val_loss: 0.2036 - val_mean_squared_error: 0.2036\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.1936 - mean_squared_error: 0.1936 - val_loss: 0.2035 - val_mean_squared_error: 0.2035\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.1933 - mean_squared_error: 0.1933 - val_loss: 0.2035 - val_mean_squared_error: 0.2035\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.1928 - mean_squared_error: 0.1928 - val_loss: 0.2036 - val_mean_squared_error: 0.2036\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.1930 - mean_squared_error: 0.1930 - val_loss: 0.2035 - val_mean_squared_error: 0.2035\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.1925 - mean_squared_error: 0.1925 - val_loss: 0.2036 - val_mean_squared_error: 0.2036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd97c7ba990>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for LSTM with hourly intervals\n",
    "lstm_data_hour = prepare_lstm_data(df_selected, time_intervals['hour'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_hour, targets_hour = create_lstm_sequences(lstm_data_hour, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_hour, X_test_hour, y_train_hour, y_test_hour = train_test_split(sequences_hour, targets_hour, test_size=0.3, random_state=1)\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model_hour = Sequential()\n",
    "lstm_model_hour.add(LSTM(20, input_shape=(X_train_hour.shape[1], X_train_hour.shape[2])))\n",
    "lstm_model_hour.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_hour.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model_hour.fit(X_train_hour, y_train_hour, epochs=20, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 0.1920\n",
      "\n",
      "1/1 [==============================] - 0s 401ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Predicted Failures for the Next 7 Hours:\n",
      "[0.44338006 0.44338036 0.44338274 0.44339988 0.44352636 0.44445175\n",
      " 0.45080683]\n",
      "Mean Squared Error for Predictions: 0.2300\n",
      "Mean Absolute Error for Predictions: 0.4769\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squared Error\n",
    "mse_hour = lstm_model_hour.evaluate(X_test_hour, y_test_hour, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mse_hour:.4f}\\n')\n",
    "\n",
    "input_data = X_test_hour[3]  # Can be any valid starting point\n",
    "\n",
    "# Make predictions\n",
    "predicted_failures = predict_future_failures(lstm_model_hour, input_data, sequence_length, prediction_steps)\n",
    "\n",
    "# Denormalize the predicted failures \n",
    "predicted_failures_denormalized = predicted_failures * (lstm_data_hour['target'].max() - lstm_data_hour['target'].min()) + lstm_data_hour['target'].min()\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted Failures for the Next 7 Hours:\")\n",
    "print(predicted_failures_denormalized)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures - y_test_hour[3:3+prediction_steps])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures - y_test_hour[3:3+prediction_steps]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 minutes -> 7 minutes - Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3258/3258 [==============================] - 59s 18ms/step - loss: 0.2015 - mean_squared_error: 0.2015 - val_loss: 0.1972 - val_mean_squared_error: 0.1972\n",
      "Epoch 2/5\n",
      "3258/3258 [==============================] - 59s 18ms/step - loss: 0.1975 - mean_squared_error: 0.1975 - val_loss: 0.1972 - val_mean_squared_error: 0.1972\n",
      "Epoch 3/5\n",
      "3258/3258 [==============================] - 58s 18ms/step - loss: 0.1975 - mean_squared_error: 0.1975 - val_loss: 0.1972 - val_mean_squared_error: 0.1972\n",
      "Epoch 4/5\n",
      "3258/3258 [==============================] - 58s 18ms/step - loss: 0.1975 - mean_squared_error: 0.1975 - val_loss: 0.1972 - val_mean_squared_error: 0.1972\n",
      "Epoch 5/5\n",
      "3258/3258 [==============================] - 57s 18ms/step - loss: 0.1975 - mean_squared_error: 0.1975 - val_loss: 0.1971 - val_mean_squared_error: 0.1971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd9681bdb10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for LSTM with minute intervals\n",
    "lstm_data_minute = prepare_lstm_data(df_selected, time_intervals['minute'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_minute, targets_minute = create_lstm_sequences(lstm_data_minute, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_minute, X_test_minute, y_train_minute, y_test_minute = train_test_split(sequences_minute, targets_minute, test_size=0.3, random_state=1)\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model_minute = Sequential()\n",
    "\n",
    "# 50 -> 10\n",
    "lstm_model_minute.add(LSTM(10, input_shape=(X_train_minute.shape[1], X_train_minute.shape[2])))\n",
    "lstm_model_minute.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_minute.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model_minute.fit(X_train_minute, y_train_minute, epochs=5, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 0.1972\n",
      "\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted failures for the next 7 minutes:\n",
      "[0.52470654 0.52470654 0.52470654 0.52470654 0.52470654 0.52470654\n",
      " 0.52470654]\n",
      "Mean Squared Error for Predictions: 0.2541\n",
      "\n",
      "Mean Absolute Error for Predictions: 0.5035\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squared Error\n",
    "mse_minute = lstm_model_minute.evaluate(X_test_minute, y_test_minute, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mse_minute:.4f}\\n')\n",
    "\n",
    "input_data = X_test_minute[3]  # Can be any valid starting point\n",
    "\n",
    "# Make predictions\n",
    "predicted_failures = predict_future_failures(lstm_model_minute, input_data, sequence_length, prediction_steps)\n",
    "\n",
    "# Denormalize the predicted failures\n",
    "predicted_failures_denormalized = predicted_failures * (lstm_data_minute['target'].max() - lstm_data_minute['target'].min()) + lstm_data_minute['target'].min()\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted failures for the next 7 minutes:\")\n",
    "print(predicted_failures_denormalized)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures - y_test_minute[3:3+prediction_steps])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures - y_test_minute[3:3+prediction_steps]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
