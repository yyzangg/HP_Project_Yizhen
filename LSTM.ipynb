{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"myData2.parquet\"\n",
    "df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info(max_cols=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 12:02:06.263316: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-12 12:02:06.311961: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-12 12:02:06.312002: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-12 12:02:06.312035: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-12 12:02:06.320967: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-12 12:02:07.210525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['timestamp_seconds', # lowers the accuracy \n",
    "                     'node_memory_Percpu_bytes', \n",
    "                     'node_context_switches_total', \n",
    "                     'surfsara_power_usage', \n",
    "                     'node_netstat_Tcp_InSegs', \n",
    "                     'node_netstat_Tcp_OutSegs', \n",
    "                     'node_network_transmit_packets_total-sum', \n",
    "                     'node_filesystem_size_bytes-sum', \n",
    "                     'node_filesystem_files-sum', \n",
    "                     'node_memory_MemFree_bytes', \n",
    "                     'node_netstat_Tcp_InErrs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'failed_jobs' representing the target variable\n",
    "df['failed_jobs'] = (df['state'] == 'FAILED').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_selected = df[['timestamp', 'state'] + selected_features].copy()\n",
    "\n",
    "# Encode the target variable 'state' to binary (0 for \"COMPLETED\", 1 otherwise)\n",
    "df_selected['target'] = (df_selected['state'] != 'COMPLETED').astype(int)\n",
    "\n",
    "# Drop the original 'state' column\n",
    "df_selected.drop('state', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time intervals\n",
    "time_intervals = {'minute': '1T', 'hour': '1H', 'day': '1D'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize selected features\n",
    "scaler = MinMaxScaler()\n",
    "df_selected[selected_features] = scaler.fit_transform(df_selected[selected_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for LSTM\n",
    "def prepare_lstm_data(data, time_interval):\n",
    "    data.set_index('timestamp', inplace=True) # FixMe\n",
    "    data_resampled = data.resample(time_interval).sum()\n",
    "    data_resampled['target'] = data_resampled['target'].clip(upper=1)  # Clip values to 1\n",
    "    return data_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for LSTM\n",
    "def create_lstm_sequences(data, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length].values\n",
    "        target = data.iloc[i+sequence_length]['target']\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sequence length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 hours -> 7 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM with hourly intervals\n",
    "lstm_data_hour = prepare_lstm_data(df_selected, time_intervals['hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences and targets\n",
    "sequences_hour, targets_hour = create_lstm_sequences(lstm_data_hour, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_hour, X_test_hour, y_train_hour, y_test_hour = train_test_split(sequences_hour, targets_hour, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "lstm_model_hour = Sequential()\n",
    "lstm_model_hour.add(LSTM(50, input_shape=(X_train_hour.shape[1], X_train_hour.shape[2])))\n",
    "lstm_model_hour.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_hour.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "54/54 [==============================] - 4s 32ms/step - loss: 0.4064 - mean_absolute_error: 0.4064 - val_loss: 0.3533 - val_mean_absolute_error: 0.3533\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 1s 23ms/step - loss: 0.3416 - mean_absolute_error: 0.3416 - val_loss: 0.3033 - val_mean_absolute_error: 0.3033\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.2897 - mean_absolute_error: 0.2897 - val_loss: 0.2593 - val_mean_absolute_error: 0.2593\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 0.2532 - mean_absolute_error: 0.2532 - val_loss: 0.2164 - val_mean_absolute_error: 0.2164\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.2240 - mean_absolute_error: 0.2240 - val_loss: 0.1915 - val_mean_absolute_error: 0.1915\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.2040 - mean_absolute_error: 0.2040 - val_loss: 0.1752 - val_mean_absolute_error: 0.1752\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1902 - mean_absolute_error: 0.1902 - val_loss: 0.1565 - val_mean_absolute_error: 0.1565\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1856 - mean_absolute_error: 0.1856 - val_loss: 0.1690 - val_mean_absolute_error: 0.1690\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 0.1737 - mean_absolute_error: 0.1737 - val_loss: 0.1533 - val_mean_absolute_error: 0.1533\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1650 - mean_absolute_error: 0.1650 - val_loss: 0.1394 - val_mean_absolute_error: 0.1394\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 0.1544 - mean_absolute_error: 0.1544 - val_loss: 0.1390 - val_mean_absolute_error: 0.1390\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1468 - mean_absolute_error: 0.1468 - val_loss: 0.1338 - val_mean_absolute_error: 0.1338\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1380 - mean_absolute_error: 0.1380 - val_loss: 0.1261 - val_mean_absolute_error: 0.1261\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1335 - mean_absolute_error: 0.1335 - val_loss: 0.1212 - val_mean_absolute_error: 0.1212\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 0.1289 - mean_absolute_error: 0.1289 - val_loss: 0.1129 - val_mean_absolute_error: 0.1129\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1275 - mean_absolute_error: 0.1275 - val_loss: 0.1020 - val_mean_absolute_error: 0.1020\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1238 - mean_absolute_error: 0.1238 - val_loss: 0.1079 - val_mean_absolute_error: 0.1079\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1176 - mean_absolute_error: 0.1176 - val_loss: 0.0953 - val_mean_absolute_error: 0.0953\n",
      "Epoch 19/20\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 0.1200 - mean_absolute_error: 0.1200 - val_loss: 0.1033 - val_mean_absolute_error: 0.1033\n",
      "Epoch 20/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1140 - mean_absolute_error: 0.1140 - val_loss: 0.0899 - val_mean_absolute_error: 0.0899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f977fbcb450>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "lstm_model_hour.fit(X_train_hour, y_train_hour, epochs=20, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Absolute Error: 0.1012\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Absolute Error\n",
    "mae = lstm_model_hour.evaluate(X_test_hour, y_test_hour, verbose=0)[1]\n",
    "print(f'Model Mean Absolute Error: {mae:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# loss, accuracy = lstm_model_hour.evaluate(X_test_hour, y_test_hour)\n",
    "# print(f'Model Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new data\n",
    "def predict_future_failures(model, input_data, sequence_length, prediction_steps):\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(prediction_steps):\n",
    "        # Make a prediction for the next time step\n",
    "        prediction = model.predict(input_data.reshape(1, sequence_length, input_data.shape[1]))\n",
    "        predictions.append(prediction[0, 0])\n",
    "\n",
    "        # Shift the input data by one time step and append the new prediction\n",
    "        input_data = np.roll(input_data, shift=-1, axis=0)\n",
    "        input_data[-1, -1] = prediction[0, 0]\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted Failures for the Next 7 Hours:\n",
      "[0.99896365 0.9987356  0.99866486 0.99865806 0.998658   0.99863833\n",
      " 0.99768925]\n",
      "Mean Absolute Error for Predictions: 0.4286\n"
     ]
    }
   ],
   "source": [
    "input_data = X_test_hour[160]  # Can be any valid starting point\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps = 7\n",
    "\n",
    "# Make predictions\n",
    "predicted_failures = predict_future_failures(lstm_model_hour, input_data, sequence_length, prediction_steps)\n",
    "\n",
    "# Denormalize the predicted failures \n",
    "predicted_failures_denormalized = predicted_failures * (lstm_data_hour['target'].max() - lstm_data_hour['target'].min()) + lstm_data_hour['target'].min()\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted Failures for the Next 7 Hours:\")\n",
    "print(predicted_failures_denormalized)\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures - y_test_hour[160:160+prediction_steps]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 minutes -> 7 minutes - Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM with minute intervals\n",
    "lstm_data_minute = prepare_lstm_data(df_selected, time_intervals['minute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences and targets\n",
    "sequences_minute, targets_minute = create_lstm_sequences(lstm_data_minute, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_minute, X_test_minute, y_train_minute, y_test_minute = train_test_split(sequences_minute, targets_minute, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "lstm_model_minute = Sequential()\n",
    "# 50 -> 20\n",
    "lstm_model_minute.add(LSTM(20, input_shape=(X_train_minute.shape[1], X_train_minute.shape[2])))\n",
    "lstm_model_minute.add(Dropout(0.2)) # FixMe\n",
    "lstm_model_minute.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_minute.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3258/3258 [==============================] - 62s 18ms/step - loss: 0.0337 - accuracy: 0.9915 - val_loss: 0.0112 - val_accuracy: 0.9977\n",
      "Epoch 2/5\n",
      "3258/3258 [==============================] - 59s 18ms/step - loss: 0.0110 - accuracy: 0.9980 - val_loss: 0.0090 - val_accuracy: 0.9984\n",
      "Epoch 3/5\n",
      "3258/3258 [==============================] - 59s 18ms/step - loss: 0.0098 - accuracy: 0.9984 - val_loss: 0.0091 - val_accuracy: 0.9984\n",
      "Epoch 4/5\n",
      "3258/3258 [==============================] - 59s 18ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0086 - val_accuracy: 0.9986\n",
      "Epoch 5/5\n",
      "3258/3258 [==============================] - 60s 18ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0088 - val_accuracy: 0.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9e56a58390>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "lstm_model_minute.fit(X_train_minute, y_train_minute, epochs=5, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1552/1552 [==============================] - 10s 6ms/step - loss: 0.0079 - accuracy: 0.9987\n",
      "Model accuracy (minute): 99.87%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_minute.evaluate(X_test_minute, y_test_minute)\n",
    "print(f'Model accuracy (minute): {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 411ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted failures for the next 7 minutes:\n",
      "[0.00059654 0.00060795 0.00062316 0.00061783 0.0006206  0.00061952\n",
      " 0.00062073]\n"
     ]
    }
   ],
   "source": [
    "input_data = X_test_minute[200]  # Can be any valid starting point\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps = 7\n",
    "\n",
    "# Make predictions\n",
    "predicted_failures = predict_future_failures(lstm_model_minute, input_data, sequence_length, prediction_steps)\n",
    "\n",
    "# Denormalize the predicted failures\n",
    "predicted_failures_denormalized = predicted_failures * (lstm_data_minute['target'].max() - lstm_data_minute['target'].min()) + lstm_data_minute['target'].min()\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted failures for the next 7 minutes:\")\n",
    "print(predicted_failures_denormalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 days -> 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM with daily intervals\n",
    "lstm_data_day = prepare_lstm_data(df_selected, time_intervals['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm_data_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 3s 523ms/step - loss: 0.6139 - accuracy: 0.7037 - val_loss: 0.3740 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6136 - accuracy: 0.7037 - val_loss: 0.3704 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6126 - accuracy: 0.7037 - val_loss: 0.3710 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6120 - accuracy: 0.7037 - val_loss: 0.3697 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6113 - accuracy: 0.7037 - val_loss: 0.3694 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6108 - accuracy: 0.7037 - val_loss: 0.3691 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6103 - accuracy: 0.7037 - val_loss: 0.3680 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6097 - accuracy: 0.7037 - val_loss: 0.3665 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6092 - accuracy: 0.7037 - val_loss: 0.3644 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6087 - accuracy: 0.7037 - val_loss: 0.3625 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6084 - accuracy: 0.7037 - val_loss: 0.3605 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6077 - accuracy: 0.7037 - val_loss: 0.3596 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6074 - accuracy: 0.7037 - val_loss: 0.3591 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.6068 - accuracy: 0.7037 - val_loss: 0.3574 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6063 - accuracy: 0.7037 - val_loss: 0.3565 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6059 - accuracy: 0.7037 - val_loss: 0.3561 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6053 - accuracy: 0.7037 - val_loss: 0.3555 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6048 - accuracy: 0.7037 - val_loss: 0.3551 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6044 - accuracy: 0.7037 - val_loss: 0.3553 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6039 - accuracy: 0.7037 - val_loss: 0.3557 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5010 - accuracy: 0.8077\n",
      "Model accuracy (day): 80.77%\n",
      "1/1 [==============================] - 0s 377ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted failures for the next 7 days:\n",
      "[0.69811136 0.69811136 0.6981113  0.69811124 0.6981105  0.69810534\n",
      " 0.6980673 ]\n"
     ]
    }
   ],
   "source": [
    "# Create sequences and targets\n",
    "sequences_day, targets_day = create_lstm_sequences(lstm_data_day, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_day, X_test_day, y_train_day, y_test_day = train_test_split(sequences_day, targets_day, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model_day = Sequential()\n",
    "lstm_model_day.add(LSTM(50, input_shape=(X_train_day.shape[1], X_train_day.shape[2])))\n",
    "lstm_model_day.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_day.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model_day.fit(X_train_day, y_train_day, epochs=20, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_day.evaluate(X_test_day, y_test_day)\n",
    "print(f'Model accuracy (day): {accuracy * 100:.2f}%')\n",
    "\n",
    "input_data = X_test_day[0]  # Can be any valid starting point\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps = 7\n",
    "\n",
    "# Make predictions\n",
    "predicted_failures = predict_future_failures(lstm_model_day, input_data, sequence_length, prediction_steps)\n",
    "\n",
    "# Denormalize the predicted failures \n",
    "predicted_failures_denormalized = predicted_failures * (lstm_data_day['target'].max() - lstm_data_day['target'].min()) + lstm_data_day['target'].min()\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted failures for the next 7 days:\")\n",
    "print(predicted_failures_denormalized)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
