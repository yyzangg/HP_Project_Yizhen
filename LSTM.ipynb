{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"myData2.parquet\"\n",
    "df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info(max_cols=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 13:46:58.273634: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-16 13:46:58.317395: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-16 13:46:58.317431: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-16 13:46:58.317459: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-16 13:46:58.325687: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-16 13:46:59.135772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['timestamp_seconds', # lowers the accuracy \n",
    "                     'node_memory_Percpu_bytes', \n",
    "                     'node_context_switches_total', \n",
    "                     'surfsara_power_usage', \n",
    "                     'node_netstat_Tcp_InSegs', \n",
    "                     'node_netstat_Tcp_OutSegs', \n",
    "                     'node_network_transmit_packets_total-sum', \n",
    "                     'node_filesystem_size_bytes-sum', \n",
    "                     'node_filesystem_files-sum', \n",
    "                     'node_memory_MemFree_bytes', \n",
    "                     'node_netstat_Tcp_InErrs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add a new column 'failed_jobs' representing the target variable\n",
    "# df['failed_jobs'] = (df['state'] == 'FAILED').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_selected = df[['timestamp', 'state'] + selected_features].copy()\n",
    "\n",
    "# Encode the target variable 'state' to binary (0 for \"COMPLETED\", 1 otherwise)\n",
    "df_selected['target'] = (df_selected['state'] != 'COMPLETED').astype(int)\n",
    "\n",
    "# Drop the original 'state' column\n",
    "df_selected.drop('state', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time intervals\n",
    "time_intervals = {'minute': '1T', 'hour': '1H', 'day': '1D'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize selected features\n",
    "scaler = MinMaxScaler()\n",
    "df_selected[selected_features] = scaler.fit_transform(df_selected[selected_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for LSTM\n",
    "def prepare_lstm_data(data, time_interval):\n",
    "    data.set_index('timestamp', inplace=True) # FixMe\n",
    "    data_resampled = data.resample(time_interval).sum()\n",
    "    data_resampled['target'] = data_resampled['target'].clip(upper=1)  # Clip values to 1\n",
    "    return data_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for LSTM\n",
    "def create_lstm_sequences(data, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length].values\n",
    "        target = data.iloc[i+sequence_length]['target']\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sequence length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 hours -> 7 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM with hourly intervals\n",
    "lstm_data_hour = prepare_lstm_data(df_selected, time_intervals['hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences and targets\n",
    "sequences_hour, targets_hour = create_lstm_sequences(lstm_data_hour, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_hour, X_test_hour, y_train_hour, y_test_hour = train_test_split(sequences_hour, targets_hour, test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "lstm_model_hour = Sequential()\n",
    "lstm_model_hour.add(LSTM(50, input_shape=(X_train_hour.shape[1], X_train_hour.shape[2])))\n",
    "lstm_model_hour.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_hour.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "54/54 [==============================] - 4s 32ms/step - loss: 0.4225 - mean_absolute_error: 0.4225 - val_loss: 0.3552 - val_mean_absolute_error: 0.3552\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.3296 - mean_absolute_error: 0.3296 - val_loss: 0.3010 - val_mean_absolute_error: 0.3010\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.2885 - mean_absolute_error: 0.2885 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.2535 - mean_absolute_error: 0.2535 - val_loss: 0.2254 - val_mean_absolute_error: 0.2254\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.2309 - mean_absolute_error: 0.2309 - val_loss: 0.2054 - val_mean_absolute_error: 0.2054\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.2104 - mean_absolute_error: 0.2104 - val_loss: 0.1899 - val_mean_absolute_error: 0.1899\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1870 - mean_absolute_error: 0.1870 - val_loss: 0.1688 - val_mean_absolute_error: 0.1688\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1721 - mean_absolute_error: 0.1721 - val_loss: 0.1639 - val_mean_absolute_error: 0.1639\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1647 - mean_absolute_error: 0.1647 - val_loss: 0.1517 - val_mean_absolute_error: 0.1517\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1562 - mean_absolute_error: 0.1562 - val_loss: 0.1518 - val_mean_absolute_error: 0.1518\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1484 - mean_absolute_error: 0.1484 - val_loss: 0.1394 - val_mean_absolute_error: 0.1394\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1444 - mean_absolute_error: 0.1444 - val_loss: 0.1387 - val_mean_absolute_error: 0.1387\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1421 - mean_absolute_error: 0.1421 - val_loss: 0.1471 - val_mean_absolute_error: 0.1471\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1447 - mean_absolute_error: 0.1447 - val_loss: 0.1247 - val_mean_absolute_error: 0.1247\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1341 - mean_absolute_error: 0.1341 - val_loss: 0.1335 - val_mean_absolute_error: 0.1335\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 0.1329 - mean_absolute_error: 0.1329 - val_loss: 0.1213 - val_mean_absolute_error: 0.1213\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1304 - mean_absolute_error: 0.1304 - val_loss: 0.1294 - val_mean_absolute_error: 0.1294\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1208 - mean_absolute_error: 0.1208 - val_loss: 0.1214 - val_mean_absolute_error: 0.1214\n",
      "Epoch 19/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1144 - mean_absolute_error: 0.1144 - val_loss: 0.1059 - val_mean_absolute_error: 0.1059\n",
      "Epoch 20/20\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 0.1155 - mean_absolute_error: 0.1155 - val_loss: 0.0988 - val_mean_absolute_error: 0.0988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f582246eb50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "lstm_model_hour.fit(X_train_hour, y_train_hour, epochs=20, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Absolute Error: 0.1097\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Absolute Error\n",
    "mae_hour = lstm_model_hour.evaluate(X_test_hour, y_test_hour, verbose=0)[1]\n",
    "print(f'Model Mean Absolute Error: {mae_hour:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# loss, accuracy = lstm_model_hour.evaluate(X_test_hour, y_test_hour)\n",
    "# print(f'Model Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new data\n",
    "def predict_future_failures(model, input_data, sequence_length, prediction_steps):\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(prediction_steps):\n",
    "        # Make a prediction for the next time step\n",
    "        prediction = model.predict(input_data.reshape(1, sequence_length, input_data.shape[1]))\n",
    "        predictions.append(prediction[0, 0])\n",
    "\n",
    "        # Shift the input data by one time step and append the new prediction\n",
    "        input_data = np.roll(input_data, shift=-1, axis=0)\n",
    "        input_data[-1, -1] = prediction[0, 0]\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 429ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted Failures for the Next 7 Hours:\n",
      "[0.9972192  0.9970992  0.99779713 0.99787205 0.99766874 0.99776\n",
      " 0.9978282 ]\n",
      "Mean Absolute Error for Predictions: 0.4291\n"
     ]
    }
   ],
   "source": [
    "input_data = X_test_hour[160]  # Can be any valid starting point\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps = 7\n",
    "\n",
    "# Make predictions\n",
    "predicted_failures = predict_future_failures(lstm_model_hour, input_data, sequence_length, prediction_steps)\n",
    "\n",
    "# Denormalize the predicted failures \n",
    "predicted_failures_denormalized = predicted_failures * (lstm_data_hour['target'].max() - lstm_data_hour['target'].min()) + lstm_data_hour['target'].min()\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted Failures for the Next 7 Hours:\")\n",
    "print(predicted_failures_denormalized)\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures - y_test_hour[160:160+prediction_steps]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 minutes -> 7 minutes - Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM with minute intervals\n",
    "lstm_data_minute = prepare_lstm_data(df_selected, time_intervals['minute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences and targets\n",
    "sequences_minute, targets_minute = create_lstm_sequences(lstm_data_minute, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_minute, X_test_minute, y_train_minute, y_test_minute = train_test_split(sequences_minute, targets_minute, test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "lstm_model_minute = Sequential()\n",
    "# 50 -> 20\n",
    "lstm_model_minute.add(LSTM(20, input_shape=(X_train_minute.shape[1], X_train_minute.shape[2])))\n",
    "lstm_model_minute.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_minute.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3258/3258 [==============================] - 61s 18ms/step - loss: 0.0249 - mean_absolute_error: 0.0249 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041\n",
      "Epoch 2/5\n",
      "3258/3258 [==============================] - 58s 18ms/step - loss: 0.0031 - mean_absolute_error: 0.0031 - val_loss: 0.0024 - val_mean_absolute_error: 0.0024\n",
      "Epoch 3/5\n",
      "3258/3258 [==============================] - 54s 16ms/step - loss: 0.0020 - mean_absolute_error: 0.0020 - val_loss: 0.0017 - val_mean_absolute_error: 0.0017\n",
      "Epoch 4/5\n",
      "3258/3258 [==============================] - 59s 18ms/step - loss: 0.0015 - mean_absolute_error: 0.0015 - val_loss: 0.0014 - val_mean_absolute_error: 0.0014\n",
      "Epoch 5/5\n",
      "3258/3258 [==============================] - 59s 18ms/step - loss: 0.0019 - mean_absolute_error: 0.0019 - val_loss: 0.0014 - val_mean_absolute_error: 0.0014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5a17dc6950>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "lstm_model_minute.fit(X_train_minute, y_train_minute, epochs=5, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Absolute Error: 0.0013\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Absolute Error\n",
    "mae_minute = lstm_model_minute.evaluate(X_test_minute, y_test_minute, verbose=0)[1]\n",
    "print(f'Model Mean Absolute Error: {mae_minute:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 421ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted failures for the next 7 minutes:\n",
      "[1.70344920e-05 1.41964401e-05 1.38251708e-05 1.43049174e-05\n",
      " 1.44723081e-05 1.43845064e-05 1.42508325e-05]\n",
      "Mean Absolute Error for Predictions: 0.2857\n"
     ]
    }
   ],
   "source": [
    "input_data = X_test_minute[200]  # Can be any valid starting point\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps = 7\n",
    "\n",
    "# Make predictions\n",
    "predicted_failures = predict_future_failures(lstm_model_minute, input_data, sequence_length, prediction_steps)\n",
    "\n",
    "# Denormalize the predicted failures\n",
    "predicted_failures_denormalized = predicted_failures * (lstm_data_minute['target'].max() - lstm_data_minute['target'].min()) + lstm_data_minute['target'].min()\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted failures for the next 7 minutes:\")\n",
    "print(predicted_failures_denormalized)\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures - y_test_hour[200:200+prediction_steps]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 days -> 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM with daily intervals\n",
    "lstm_data_day = prepare_lstm_data(df_selected, time_intervals['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm_data_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 2s 514ms/step - loss: 0.5989 - mean_absolute_error: 0.5989 - val_loss: 0.6194 - val_mean_absolute_error: 0.6194\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5828 - mean_absolute_error: 0.5828 - val_loss: 0.5862 - val_mean_absolute_error: 0.5862\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5642 - mean_absolute_error: 0.5642 - val_loss: 0.6005 - val_mean_absolute_error: 0.6005\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5533 - mean_absolute_error: 0.5533 - val_loss: 0.5698 - val_mean_absolute_error: 0.5698\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5399 - mean_absolute_error: 0.5399 - val_loss: 0.5581 - val_mean_absolute_error: 0.5581\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5235 - mean_absolute_error: 0.5235 - val_loss: 0.5360 - val_mean_absolute_error: 0.5360\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5171 - mean_absolute_error: 0.5171 - val_loss: 0.5320 - val_mean_absolute_error: 0.5320\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5094 - mean_absolute_error: 0.5094 - val_loss: 0.5259 - val_mean_absolute_error: 0.5259\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5051 - mean_absolute_error: 0.5051 - val_loss: 0.5135 - val_mean_absolute_error: 0.5135\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5001 - mean_absolute_error: 0.5001 - val_loss: 0.5100 - val_mean_absolute_error: 0.5100\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4971 - mean_absolute_error: 0.4971 - val_loss: 0.5035 - val_mean_absolute_error: 0.5035\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4845 - mean_absolute_error: 0.4845 - val_loss: 0.4984 - val_mean_absolute_error: 0.4984\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4826 - mean_absolute_error: 0.4826 - val_loss: 0.4810 - val_mean_absolute_error: 0.4810\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4781 - mean_absolute_error: 0.4781 - val_loss: 0.4657 - val_mean_absolute_error: 0.4657\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4686 - mean_absolute_error: 0.4686 - val_loss: 0.4518 - val_mean_absolute_error: 0.4518\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4515 - mean_absolute_error: 0.4515 - val_loss: 0.4324 - val_mean_absolute_error: 0.4324\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4438 - mean_absolute_error: 0.4438 - val_loss: 0.4280 - val_mean_absolute_error: 0.4280\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4371 - mean_absolute_error: 0.4371 - val_loss: 0.4239 - val_mean_absolute_error: 0.4239\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4301 - mean_absolute_error: 0.4301 - val_loss: 0.4120 - val_mean_absolute_error: 0.4120\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4234 - mean_absolute_error: 0.4234 - val_loss: 0.3983 - val_mean_absolute_error: 0.3983\n",
      "Model Mean Absolute Error: 0.4440\n",
      "1/1 [==============================] - 0s 412ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Predicted failures for the next 7 days:\n",
      "[0.3664659  0.494756   0.49171248 0.4885145  0.66487527 0.66583705\n",
      " 0.6091474 ]\n",
      "Mean Absolute Error for Predictions: 0.4598\n"
     ]
    }
   ],
   "source": [
    "# Create sequences and targets\n",
    "sequences_day, targets_day = create_lstm_sequences(lstm_data_day, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_day, X_test_day, y_train_day, y_test_day = train_test_split(sequences_day, targets_day, test_size=0.3, random_state=1)\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model_day = Sequential()\n",
    "lstm_model_day.add(LSTM(50, input_shape=(X_train_day.shape[1], X_train_day.shape[2])))\n",
    "lstm_model_day.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_day.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model_day.fit(X_train_day, y_train_day, epochs=20, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model using Mean Absolute Error\n",
    "mae_day = lstm_model_day.evaluate(X_test_day, y_test_day, verbose=0)[1]\n",
    "print(f'Model Mean Absolute Error: {mae_day:.4f}')\n",
    "\n",
    "input_data = X_test_day[0]  # Can be any valid starting point\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps = 7\n",
    "\n",
    "# Make predictions\n",
    "predicted_failures = predict_future_failures(lstm_model_day, input_data, sequence_length, prediction_steps)\n",
    "\n",
    "# Denormalize the predicted failures \n",
    "predicted_failures_denormalized = predicted_failures * (lstm_data_day['target'].max() - lstm_data_day['target'].min()) + lstm_data_day['target'].min()\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted failures for the next 7 days:\")\n",
    "print(predicted_failures_denormalized)\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures - y_test_day[0:0+prediction_steps]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
