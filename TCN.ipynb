{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 11:32:48.024185: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-28 11:32:48.066793: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-28 11:32:48.066828: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-28 11:32:48.066849: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-28 11:32:48.074734: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-28 11:32:48.883281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tcn import TCN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ML_Node'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['timestamp_seconds', # lowers the accuracy \n",
    "                     'node_memory_Percpu_bytes', \n",
    "                     'node_context_switches_total', \n",
    "                     'surfsara_power_usage', \n",
    "                     'node_netstat_Tcp_InSegs', \n",
    "                     'node_netstat_Tcp_OutSegs', \n",
    "                     'node_network_transmit_packets_total-sum', \n",
    "                     'node_filesystem_size_bytes-sum', \n",
    "                     'node_filesystem_files-sum', \n",
    "                     'node_memory_MemFree_bytes', \n",
    "                     'node_netstat_Tcp_InErrs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"myData2.parquet\"\n",
    "df = pd.read_parquet(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_selected = df[['timestamp', 'state'] + selected_features].copy()\n",
    "\n",
    "# Encode the target variable 'state' to binary (0 for \"COMPLETED\", 1 otherwise)\n",
    "df_selected['target'] = (df_selected['state'] != 'COMPLETED').astype(int)\n",
    "\n",
    "# Drop the original 'state' column\n",
    "df_selected.drop('state', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time intervals\n",
    "time_intervals = {'minute': '1T', 'hour': '1H', 'day': '1D'}\n",
    "\n",
    "# Normalize selected features\n",
    "scaler = MinMaxScaler()\n",
    "df_selected[selected_features] = scaler.fit_transform(df_selected[selected_features])\n",
    "\n",
    "# Set sequence length\n",
    "sequence_length = 30\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps_tcn = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for TCN\n",
    "def prepare_tcn_data(data, time_interval):\n",
    "    data.set_index('timestamp', inplace=True)\n",
    "    data_resampled = data.resample(time_interval).sum()\n",
    "    # data_resampled['target'] = data_resampled['target'].clip(upper=1)  # Clip values to 1\n",
    "    return data_resampled\n",
    "\n",
    "# Function to create sequences for TCN\n",
    "def create_tcn_sequences(data, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length].values\n",
    "        target = data.iloc[i+sequence_length]['target']\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Function to make predictions on new data for TCN model\n",
    "def predict_future_failures_tcn(model, input_data, sequence_length, prediction_steps):\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(prediction_steps):\n",
    "        # Make a prediction for the next time step\n",
    "        prediction = model.predict(input_data.reshape(1, sequence_length, input_data.shape[1]))\n",
    "        predictions.append(prediction[0, 0])\n",
    "\n",
    "        # Shift the input data by one time step and append the new prediction\n",
    "        input_data = np.roll(input_data, shift=-1, axis=0)\n",
    "        input_data[-1, -1] = prediction[0, 0]\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_selected = df[['timestamp', 'state'] + selected_features].copy()\n",
    "\n",
    "# Encode the target variable 'state' to binary (0 for \"COMPLETED\", 1 otherwise)\n",
    "df_selected['target'] = (df_selected['state'] != 'COMPLETED').astype(int)\n",
    "\n",
    "# Drop the original 'state' column\n",
    "df_selected.drop('state', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 3s 514ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 6988847.0000 - mean_squared_error: 6988847.0000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 6988846.5000 - mean_squared_error: 6988846.5000 - val_loss: 13315637.0000 - val_mean_squared_error: 13315637.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f4c1df0cf50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for TCN with day intervals\n",
    "tcn_data_day = prepare_tcn_data(df_selected, time_intervals['day'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_day, targets_day = create_tcn_sequences(tcn_data_day, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_day, X_test_day, y_train_day, y_test_day = train_test_split(sequences_day, targets_day, test_size=0.3, random_state=1)\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model_day = Sequential([\n",
    "    TCN(input_shape=(sequence_length, X_train_day.shape[2])),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tcn_model_day.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "tcn_model_day.fit(X_train_day, y_train_day, epochs=20, batch_size=32, validation_split=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 0.1923\n",
      "\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted Failures for the Next 7 Time Steps (TCN):\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Mean Squared Error for Predictions: 0.0000\n",
      "Mean Absolute Error for Predictions: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squred Error\n",
    "mae_day = tcn_model_day.evaluate(X_test_day, y_test_day, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mae_day:.4f}\\n')\n",
    "\n",
    "# Select a starting point for predictions\n",
    "input_data_tcn = X_test_day[3]\n",
    "\n",
    "# Make predictions with the TCN model\n",
    "predicted_failures_tcn = predict_future_failures_tcn(tcn_model_day, input_data_tcn, sequence_length, prediction_steps_tcn)\n",
    "\n",
    "# Print the predicted failures for TCN\n",
    "print(\"Predicted Failures for the Next 7 Time Steps (TCN):\")\n",
    "print(predicted_failures_tcn)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_tcn - y_test_day[3:3+prediction_steps_tcn])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures_tcn - y_test_day[3:3+prediction_steps_tcn]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "51/51 [==============================] - 6s 53ms/step - loss: 0.4542 - mean_squared_error: 0.4542 - val_loss: 0.4129 - val_mean_squared_error: 0.4129\n",
      "Epoch 2/20\n",
      "51/51 [==============================] - 2s 46ms/step - loss: 0.4542 - mean_squared_error: 0.4542 - val_loss: 0.4128 - val_mean_squared_error: 0.4128\n",
      "Epoch 3/20\n",
      "51/51 [==============================] - 2s 45ms/step - loss: 0.4542 - mean_squared_error: 0.4542 - val_loss: 0.4128 - val_mean_squared_error: 0.4128\n",
      "Epoch 4/20\n",
      "51/51 [==============================] - 2s 44ms/step - loss: 0.4542 - mean_squared_error: 0.4542 - val_loss: 0.4128 - val_mean_squared_error: 0.4128\n",
      "Epoch 5/20\n",
      "51/51 [==============================] - 2s 44ms/step - loss: 0.4541 - mean_squared_error: 0.4541 - val_loss: 0.4127 - val_mean_squared_error: 0.4127\n",
      "Epoch 6/20\n",
      "51/51 [==============================] - 2s 43ms/step - loss: 0.4541 - mean_squared_error: 0.4541 - val_loss: 0.4127 - val_mean_squared_error: 0.4127\n",
      "Epoch 7/20\n",
      "51/51 [==============================] - 2s 43ms/step - loss: 0.4541 - mean_squared_error: 0.4541 - val_loss: 0.4127 - val_mean_squared_error: 0.4127\n",
      "Epoch 8/20\n",
      "51/51 [==============================] - 2s 45ms/step - loss: 0.4541 - mean_squared_error: 0.4541 - val_loss: 0.4126 - val_mean_squared_error: 0.4126\n",
      "Epoch 9/20\n",
      "51/51 [==============================] - 2s 44ms/step - loss: 0.4540 - mean_squared_error: 0.4540 - val_loss: 0.4126 - val_mean_squared_error: 0.4126\n",
      "Epoch 10/20\n",
      "51/51 [==============================] - 2s 43ms/step - loss: 0.4540 - mean_squared_error: 0.4540 - val_loss: 0.4126 - val_mean_squared_error: 0.4126\n",
      "Epoch 11/20\n",
      "51/51 [==============================] - 2s 45ms/step - loss: 0.4540 - mean_squared_error: 0.4540 - val_loss: 0.4125 - val_mean_squared_error: 0.4125\n",
      "Epoch 12/20\n",
      "51/51 [==============================] - 2s 43ms/step - loss: 0.4540 - mean_squared_error: 0.4540 - val_loss: 0.4125 - val_mean_squared_error: 0.4125\n",
      "Epoch 13/20\n",
      "51/51 [==============================] - 2s 43ms/step - loss: 0.4539 - mean_squared_error: 0.4539 - val_loss: 0.4125 - val_mean_squared_error: 0.4125\n",
      "Epoch 14/20\n",
      "51/51 [==============================] - 2s 43ms/step - loss: 0.4539 - mean_squared_error: 0.4539 - val_loss: 0.4124 - val_mean_squared_error: 0.4124\n",
      "Epoch 15/20\n",
      "51/51 [==============================] - 2s 44ms/step - loss: 0.4539 - mean_squared_error: 0.4539 - val_loss: 0.4124 - val_mean_squared_error: 0.4124\n",
      "Epoch 16/20\n",
      "51/51 [==============================] - 2s 43ms/step - loss: 0.4539 - mean_squared_error: 0.4539 - val_loss: 0.4124 - val_mean_squared_error: 0.4124\n",
      "Epoch 17/20\n",
      "51/51 [==============================] - 2s 43ms/step - loss: 0.4538 - mean_squared_error: 0.4538 - val_loss: 0.4124 - val_mean_squared_error: 0.4124\n",
      "Epoch 18/20\n",
      "51/51 [==============================] - 2s 43ms/step - loss: 0.4538 - mean_squared_error: 0.4538 - val_loss: 0.4123 - val_mean_squared_error: 0.4123\n",
      "Epoch 19/20\n",
      "51/51 [==============================] - 2s 42ms/step - loss: 0.4538 - mean_squared_error: 0.4538 - val_loss: 0.4123 - val_mean_squared_error: 0.4123\n",
      "Epoch 20/20\n",
      "51/51 [==============================] - 2s 44ms/step - loss: 0.4538 - mean_squared_error: 0.4538 - val_loss: 0.4123 - val_mean_squared_error: 0.4123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7faa5145cbd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for TCN\n",
    "tcn_data_hour = prepare_tcn_data(df_selected, time_intervals['hour'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_hour, targets_hour = create_tcn_sequences(tcn_data_hour, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_hour, X_test_hour, y_train_hour, y_test_hour = train_test_split(sequences_hour, targets_hour, test_size=0.3, random_state=1)\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model_hour = Sequential([\n",
    "    TCN(input_shape=(sequence_length, X_train_hour.shape[2])),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tcn_model_hour.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "tcn_model_hour.fit(X_train_hour, y_train_hour, epochs=20, batch_size=32, validation_split=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 0.4503\n",
      "\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted Failures for the Next 7 Time Steps (TCN):\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Mean Squared Error for Predictions: 0.2857\n",
      "\n",
      "Mean Absolute Error for Predictions: 0.2857\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squred Error\n",
    "mae_hour = tcn_model_hour.evaluate(X_test_hour, y_test_hour, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mae_hour:.4f}\\n')\n",
    "\n",
    "# Select a starting point for predictions\n",
    "input_data_tcn = X_test_hour[3]\n",
    "\n",
    "# Make predictions with the TCN model\n",
    "predicted_failures_tcn = predict_future_failures_tcn(tcn_model_hour, input_data_tcn, sequence_length, prediction_steps_tcn)\n",
    "\n",
    "# Print the predicted failures for TCN\n",
    "print(\"Predicted Failures for the Next 7 Time Steps (TCN):\")\n",
    "print(predicted_failures_tcn)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_tcn - y_test_hour[3:3+prediction_steps_tcn])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures_tcn - y_test_hour[3:3+prediction_steps_tcn]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1539/1539 [==============================] - 96s 61ms/step - loss: 0.4057 - mean_squared_error: 0.4057 - val_loss: 0.3906 - val_mean_squared_error: 0.3906\n",
      "Epoch 2/10\n",
      "1539/1539 [==============================] - 92s 60ms/step - loss: 0.3882 - mean_squared_error: 0.3882 - val_loss: 0.3834 - val_mean_squared_error: 0.3834\n",
      "Epoch 3/10\n",
      "1539/1539 [==============================] - 92s 60ms/step - loss: 0.3844 - mean_squared_error: 0.3844 - val_loss: 0.3814 - val_mean_squared_error: 0.3814\n",
      "Epoch 4/10\n",
      "1539/1539 [==============================] - 93s 60ms/step - loss: 0.3833 - mean_squared_error: 0.3833 - val_loss: 0.3807 - val_mean_squared_error: 0.3807\n",
      "Epoch 5/10\n",
      "1539/1539 [==============================] - 93s 60ms/step - loss: 0.3828 - mean_squared_error: 0.3828 - val_loss: 0.3805 - val_mean_squared_error: 0.3805\n",
      "Epoch 6/10\n",
      "1539/1539 [==============================] - 93s 61ms/step - loss: 0.3826 - mean_squared_error: 0.3826 - val_loss: 0.3803 - val_mean_squared_error: 0.3803\n",
      "Epoch 7/10\n",
      "1539/1539 [==============================] - 93s 60ms/step - loss: 0.3825 - mean_squared_error: 0.3825 - val_loss: 0.3803 - val_mean_squared_error: 0.3803\n",
      "Epoch 8/10\n",
      "1539/1539 [==============================] - 93s 60ms/step - loss: 0.3825 - mean_squared_error: 0.3825 - val_loss: 0.3802 - val_mean_squared_error: 0.3802\n",
      "Epoch 9/10\n",
      "1539/1539 [==============================] - 93s 61ms/step - loss: 0.3825 - mean_squared_error: 0.3825 - val_loss: 0.3802 - val_mean_squared_error: 0.3802\n",
      "Epoch 10/10\n",
      "1539/1539 [==============================] - 93s 60ms/step - loss: 0.3825 - mean_squared_error: 0.3825 - val_loss: 0.3802 - val_mean_squared_error: 0.3802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7faa11d79e10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for TCN\n",
    "tcn_data_minute = prepare_tcn_data(df_selected, time_intervals['minute'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_minute, targets_minute = create_tcn_sequences(tcn_data_minute, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_minute, X_test_minute, y_train_minute, y_test_minute = train_test_split(sequences_minute, targets_minute, test_size=0.3, random_state=1)\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model_minute = Sequential([\n",
    "    TCN(input_shape=(sequence_length, X_train_minute.shape[2])),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tcn_model_minute.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "tcn_model_minute.fit(X_train_minute, y_train_minute, epochs=10, batch_size=64, validation_split=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 0.4503\n",
      "\n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted Failures for the Next 7 Time Steps (TCN):\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "Mean Squared Error for Predictions: 0.5714\n",
      "\n",
      "Mean Absolute Error for Predictions: 0.5714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squred Error\n",
    "mae_minute = tcn_model_minute.evaluate(X_test_minute, y_test_minute, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mae_hour:.4f}\\n')\n",
    "\n",
    "# Select a starting point for predictions\n",
    "input_data_tcn = X_test_minute[3]\n",
    "\n",
    "# Make predictions with the TCN model\n",
    "predicted_failures_tcn = predict_future_failures_tcn(tcn_model_minute, input_data_tcn, sequence_length, prediction_steps_tcn)\n",
    "\n",
    "# Print the predicted failures for TCN\n",
    "print(\"Predicted Failures for the Next 7 Time Steps (TCN):\")\n",
    "print(predicted_failures_tcn)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_tcn - y_test_minute[3:3+prediction_steps_tcn])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures_tcn - y_test_minute[3:3+prediction_steps_tcn]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
