{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tcn import TCN\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ML_Node'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['timestamp_seconds', # FixMe\n",
    "                     'node_memory_Percpu_bytes', \n",
    "                     'node_context_switches_total', \n",
    "                     'surfsara_power_usage', \n",
    "                     'node_netstat_Tcp_InSegs', \n",
    "                     'node_netstat_Tcp_OutSegs', \n",
    "                     'node_network_transmit_packets_total-sum', \n",
    "                     'node_filesystem_size_bytes-sum', \n",
    "                     'node_filesystem_files-sum', \n",
    "                     'node_memory_MemFree_bytes', \n",
    "                     'node_netstat_Tcp_InErrs']\n",
    "# FixMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"myData2.parquet\"\n",
    "df = pd.read_parquet(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time intervals\n",
    "time_intervals = {'minute': '1T', 'hour': '1H', 'day': '1D'}\n",
    "\n",
    "# Set sequence length\n",
    "sequence_length = 30\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps_tcn = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for TCN\n",
    "def prepare_tcn_data(data, time_interval):\n",
    "    data.set_index('timestamp', inplace=True)\n",
    "    data_resampled = data.resample(time_interval).sum()\n",
    "    \n",
    "    target_mean = data_resampled['target'].mean()\n",
    "    target_std = data_resampled['target'].std()\n",
    "    data_resampled['target'] = (data_resampled['target'] - target_mean) / target_std\n",
    "    \n",
    "    target_min = data_resampled['target'].min()\n",
    "    target_max = data_resampled['target'].max()\n",
    "    print(\"Minimum value of target variable:\", target_min)\n",
    "    print(\"Maximum value of target variable:\", target_max)\n",
    "    \n",
    "    return data_resampled\n",
    "\n",
    "# Function to create sequences for TCN\n",
    "def create_tcn_sequences(data, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length].values\n",
    "        target = data.iloc[i+sequence_length]['target']\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Function to make predictions on new data for TCN model\n",
    "def predict_future_failures_tcn(model, input_data, sequence_length, prediction_steps):\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(prediction_steps):\n",
    "        # Make a prediction for the next time step\n",
    "        prediction = model.predict(input_data.reshape(1, sequence_length, input_data.shape[1]))\n",
    "        predictions.append(prediction[0, 0])\n",
    "\n",
    "        # Shift the input data by one time step and append the new prediction\n",
    "        input_data = np.roll(input_data, shift=-1, axis=0)\n",
    "        input_data[-1, -1] = prediction[0, 0]\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_selected = df[['timestamp', 'state'] + selected_features].copy()\n",
    "\n",
    "# Encode the target variable 'state' to binary (0 for \"COMPLETED\", 1 otherwise)\n",
    "df_selected['target'] = (df_selected['state'] != 'COMPLETED').astype(int)\n",
    "\n",
    "# Drop the original 'state' column\n",
    "df_selected.drop('state', axis=1, inplace=True)\n",
    "\n",
    "# Normalize selected features\n",
    "scaler = MinMaxScaler()\n",
    "df_selected[selected_features] = scaler.fit_transform(df_selected[selected_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of target variable: -0.7362924419461765\n",
      "Maximum value of target variable: 5.322951443655186\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - 3s 456ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.6825 - mean_squared_error: 1.6825 - val_loss: 1.5427 - val_mean_squared_error: 1.5427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fcab0540c10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for TCN with day intervals\n",
    "tcn_data_day = prepare_tcn_data(df_selected, time_intervals['day'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_day, targets_day = create_tcn_sequences(tcn_data_day, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_day, X_test_day, y_train_day, y_test_day = train_test_split(sequences_day, targets_day, test_size=0.3, random_state=1)\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model_day = Sequential([\n",
    "    TCN(input_shape=(sequence_length, X_train_day.shape[2])),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tcn_model_day.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "history_day = tcn_model_day.fit(X_train_day, y_train_day, epochs=20, batch_size=32, validation_split=0.15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance_Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using Mean Squred Error\n",
    "mse_day = tcn_model_day.evaluate(X_test_day, y_test_day, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mse_day:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "y_pred_day = tcn_model_day.predict(X_test_day)\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "mae = mean_absolute_error(y_test_day, y_pred_day)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# Calculate Root Mean Squared Error\n",
    "rmse = mean_squared_error(y_test_day, y_pred_day, squared=False)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test_day, y_pred_day)\n",
    "print(\"R-squared (R2):\", r2)\n",
    "\n",
    "# Additional: Compare with training and validation loss from history\n",
    "train_loss = history_day.history['loss']\n",
    "val_loss = history_day.history['val_loss']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(history_day.history['loss'], label='Training Loss')\n",
    "plt.plot(history_day.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Failures vs. True Failures Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_day = 14\n",
    "\n",
    "input_data = X_test_day[index_day]  # Can be any valid starting point\n",
    "\n",
    "# Make predictions\n",
    "predicted_failures_day = predict_future_failures_tcn(tcn_model_day, input_data, sequence_length, prediction_steps_tcn)#prediction_steps\n",
    "\n",
    "# Denormalize the predicted failures \n",
    "predicted_failures_denormalized_day = np.array(predicted_failures_day) * (tcn_data_day['target'].max() - tcn_data_day['target'].min()) + tcn_data_day['target'].min()\n",
    "\n",
    "# Get the true failures for the specified number of hours\n",
    "true_failures_day = y_test_day[index_day:index_day + prediction_steps_tcn]  # Adjust the range as needed #prediction_steps\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted failures for the next 7 days:\")\n",
    "print(predicted_failures_denormalized_day)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_day - true_failures_day)**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures_day - true_failures_day))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}')\n",
    "\n",
    "# Plot predicted failures vs. true failures\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(predicted_failures_denormalized_day, label='Predicted Failures', marker='o')\n",
    "plt.plot(true_failures_day, label='True Failures', marker='x')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Number of Failures')\n",
    "plt.title('Predicted Failures vs. True Failures')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 1.9790\n",
      "\n",
      "1/1 [==============================] - 0s 325ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted Failures for the Next 7 Time Steps (TCN):\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "Mean Squared Error for Predictions: 3.9855\n",
      "\n",
      "Mean Absolute Error for Predictions: 1.6349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Select a starting point for predictions\n",
    "input_data_tcn = X_test_day[3]\n",
    "\n",
    "# Make predictions with the TCN model\n",
    "predicted_failures_tcn = predict_future_failures_tcn(tcn_model_day, input_data_tcn, sequence_length, prediction_steps_tcn)\n",
    "\n",
    "# Print the predicted failures for TCN\n",
    "print(\"Predicted Failures for the Next 7 Time Steps (TCN):\")\n",
    "print(predicted_failures_tcn)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_tcn - y_test_day[3:3 + prediction_steps_tcn])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures_tcn - y_test_day[3:3 + prediction_steps_tcn]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_selected = df[['timestamp', 'state'] + selected_features].copy()\n",
    "\n",
    "# Encode the target variable 'state' to binary (0 for \"COMPLETED\", 1 otherwise)\n",
    "df_selected['target'] = (df_selected['state'] != 'COMPLETED').astype(int)\n",
    "\n",
    "# Drop the original 'state' column\n",
    "df_selected.drop('state', axis=1, inplace=True)\n",
    "\n",
    "# Normalize selected features\n",
    "scaler = MinMaxScaler()\n",
    "df_selected[selected_features] = scaler.fit_transform(df_selected[selected_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of target variable: -0.44004356707964004\n",
      "Maximum value of target variable: 22.33986610637043\n",
      "Epoch 1/20\n",
      "51/51 [==============================] - 5s 53ms/step - loss: 0.6922 - mean_squared_error: 0.6922 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 2/20\n",
      "51/51 [==============================] - 2s 46ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 3/20\n",
      "51/51 [==============================] - 2s 43ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 4/20\n",
      "51/51 [==============================] - 2s 43ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 5/20\n",
      "51/51 [==============================] - 2s 45ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 6/20\n",
      "51/51 [==============================] - 2s 44ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 7/20\n",
      "51/51 [==============================] - 2s 43ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 8/20\n",
      "51/51 [==============================] - 2s 44ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 9/20\n",
      "51/51 [==============================] - 2s 41ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 10/20\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 11/20\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 12/20\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 13/20\n",
      "51/51 [==============================] - 2s 39ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 14/20\n",
      "51/51 [==============================] - 2s 42ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 15/20\n",
      "51/51 [==============================] - 2s 40ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 16/20\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 17/20\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 18/20\n",
      "51/51 [==============================] - 2s 37ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 19/20\n",
      "51/51 [==============================] - 2s 37ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n",
      "Epoch 20/20\n",
      "51/51 [==============================] - 2s 40ms/step - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.5196 - val_mean_squared_error: 0.5196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fcaa0d154d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for TCN\n",
    "tcn_data_hour = prepare_tcn_data(df_selected, time_intervals['hour'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_hour, targets_hour = create_tcn_sequences(tcn_data_hour, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_hour, X_test_hour, y_train_hour, y_test_hour = train_test_split(sequences_hour, targets_hour, test_size=0.3, random_state=1)\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model_hour = Sequential([\n",
    "    TCN(input_shape=(sequence_length, X_train_hour.shape[2])),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tcn_model_hour.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "tcn_model_hour.fit(X_train_hour, y_train_hour, epochs=20, batch_size=32, validation_split=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 1.8850\n",
      "\n",
      "1/1 [==============================] - 1s 720ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted Failures for the Next 7 Time Steps (TCN):\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Mean Squared Error for Predictions: 0.1671\n",
      "\n",
      "Mean Absolute Error for Predictions: 0.4041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squred Error\n",
    "mae_hour = tcn_model_hour.evaluate(X_test_hour, y_test_hour, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mae_hour:.4f}\\n')\n",
    "\n",
    "# Select a starting point for predictions\n",
    "input_data_tcn = X_test_hour[3]\n",
    "\n",
    "# Make predictions with the TCN model\n",
    "predicted_failures_tcn = predict_future_failures_tcn(tcn_model_hour, input_data_tcn, sequence_length, prediction_steps_tcn)\n",
    "\n",
    "# Print the predicted failures for TCN\n",
    "print(\"Predicted Failures for the Next 7 Time Steps (TCN):\")\n",
    "print(predicted_failures_tcn)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_tcn - y_test_hour[3:3+prediction_steps_tcn])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures_tcn - y_test_hour[3:3+prediction_steps_tcn]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_selected = df[['timestamp', 'state'] + selected_features].copy()\n",
    "\n",
    "# Encode the target variable 'state' to binary (0 for \"COMPLETED\", 1 otherwise)\n",
    "df_selected['target'] = (df_selected['state'] != 'COMPLETED').astype(int)\n",
    "\n",
    "# Drop the original 'state' column\n",
    "df_selected.drop('state', axis=1, inplace=True)\n",
    "\n",
    "# Normalize selected features\n",
    "scaler = MinMaxScaler()\n",
    "df_selected[selected_features] = scaler.fit_transform(df_selected[selected_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of target variable: -0.43499466427008204\n",
      "Maximum value of target variable: 22.077561101301857\n",
      "Epoch 1/10\n",
      "1539/1539 [==============================] - 96s 61ms/step - loss: 0.9966 - mean_squared_error: 0.9966 - val_loss: 0.9588 - val_mean_squared_error: 0.9588\n",
      "Epoch 2/10\n",
      "1539/1539 [==============================] - 93s 60ms/step - loss: 0.9966 - mean_squared_error: 0.9966 - val_loss: 0.9588 - val_mean_squared_error: 0.9588\n",
      "Epoch 3/10\n",
      "1539/1539 [==============================] - 90s 59ms/step - loss: 0.9966 - mean_squared_error: 0.9966 - val_loss: 0.9588 - val_mean_squared_error: 0.9588\n",
      "Epoch 4/10\n",
      "1539/1539 [==============================] - 92s 60ms/step - loss: 0.9966 - mean_squared_error: 0.9966 - val_loss: 0.9588 - val_mean_squared_error: 0.9588\n",
      "Epoch 5/10\n",
      "1539/1539 [==============================] - 91s 59ms/step - loss: 0.9966 - mean_squared_error: 0.9966 - val_loss: 0.9588 - val_mean_squared_error: 0.9588\n",
      "Epoch 6/10\n",
      "1539/1539 [==============================] - 90s 58ms/step - loss: 0.9966 - mean_squared_error: 0.9966 - val_loss: 0.9588 - val_mean_squared_error: 0.9588\n",
      "Epoch 7/10\n",
      "1539/1539 [==============================] - 88s 57ms/step - loss: 0.9966 - mean_squared_error: 0.9966 - val_loss: 0.9588 - val_mean_squared_error: 0.9588\n",
      "Epoch 8/10\n",
      "1539/1539 [==============================] - 89s 58ms/step - loss: 0.9966 - mean_squared_error: 0.9966 - val_loss: 0.9588 - val_mean_squared_error: 0.9588\n",
      "Epoch 9/10\n",
      "1539/1539 [==============================] - 88s 57ms/step - loss: 0.9966 - mean_squared_error: 0.9966 - val_loss: 0.9588 - val_mean_squared_error: 0.9588\n",
      "Epoch 10/10\n",
      "1539/1539 [==============================] - 88s 57ms/step - loss: 0.9966 - mean_squared_error: 0.9966 - val_loss: 0.9588 - val_mean_squared_error: 0.9588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc9c7eda550>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for TCN\n",
    "tcn_data_minute = prepare_tcn_data(df_selected, time_intervals['minute'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_minute, targets_minute = create_tcn_sequences(tcn_data_minute, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_minute, X_test_minute, y_train_minute, y_test_minute = train_test_split(sequences_minute, targets_minute, test_size=0.3, random_state=1)\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model_minute = Sequential([\n",
    "    TCN(input_shape=(sequence_length, X_train_minute.shape[2])),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tcn_model_minute.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "tcn_model_minute.fit(X_train_minute, y_train_minute, epochs=10, batch_size=64, validation_split=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 1.8850\n",
      "\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted Failures for the Next 7 Time Steps (TCN):\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Mean Squared Error for Predictions: 0.3884\n",
      "\n",
      "Mean Absolute Error for Predictions: 0.5647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squred Error\n",
    "mae_minute = tcn_model_minute.evaluate(X_test_minute, y_test_minute, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mae_hour:.4f}\\n')\n",
    "\n",
    "# Select a starting point for predictions\n",
    "input_data_tcn = X_test_minute[3]\n",
    "\n",
    "# Make predictions with the TCN model\n",
    "predicted_failures_tcn = predict_future_failures_tcn(tcn_model_minute, input_data_tcn, sequence_length, prediction_steps_tcn)\n",
    "\n",
    "# Print the predicted failures for TCN\n",
    "print(\"Predicted Failures for the Next 7 Time Steps (TCN):\")\n",
    "print(predicted_failures_tcn)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_tcn - y_test_minute[3:3+prediction_steps_tcn])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures_tcn - y_test_minute[3:3+prediction_steps_tcn]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
