{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 20:45:13.343563: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-11 20:45:13.387299: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-11 20:45:13.387331: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-11 20:45:13.387354: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-11 20:45:13.395506: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 20:45:14.289400: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tcn import TCN\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Concatenate, Attention, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"myData2.parquet\"\n",
    "df = pd.read_parquet(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['timestamp_seconds', \n",
    "                     'node_memory_Percpu_bytes', \n",
    "                     'node_context_switches_total', \n",
    "                     'surfsara_power_usage', \n",
    "                     'node_netstat_Tcp_InSegs', \n",
    "                     'node_netstat_Tcp_OutSegs', \n",
    "                     'node_network_transmit_packets_total-sum', \n",
    "                     'node_filesystem_size_bytes-sum', \n",
    "                     'node_filesystem_files-sum', \n",
    "                     'node_memory_MemFree_bytes', \n",
    "                     'node_netstat_Tcp_InErrs']\n",
    "# FixMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time intervals\n",
    "time_intervals = {'minute': '1T', 'hour': '1H', 'day': '1D'}\n",
    "\n",
    "# Set sequence length\n",
    "sequence_length = 30\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps = 7\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data\n",
    "def prepare_data(data, time_interval):\n",
    "    data.set_index('timestamp', inplace=True)\n",
    "    data_resampled = data.resample(time_interval).sum()\n",
    "    \n",
    "    target_mean = data_resampled['target'].mean()\n",
    "    target_std = data_resampled['target'].std()\n",
    "    data_resampled['target'] = (data_resampled['target'] - target_mean) / target_std\n",
    "    \n",
    "    target_min = data_resampled['target'].min()\n",
    "    target_max = data_resampled['target'].max()\n",
    "    print(\"Minimum value of target variable:\", target_min)\n",
    "    print(\"Maximum value of target variable:\", target_max)\n",
    "    \n",
    "    return data_resampled\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length].values\n",
    "        target = data.iloc[i+sequence_length]['target']\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a hybrid model with attention mechanism\n",
    "def create_attention_hybrid_model(lstm_model, tcn_model):\n",
    "    lstm_input = lstm_model.input\n",
    "    tcn_input = tcn_model.input\n",
    "\n",
    "    # Get the output layers of both models\n",
    "    lstm_output = lstm_model.layers[-1].output\n",
    "    tcn_output = tcn_model.layers[-1].output\n",
    "\n",
    "    # Use Attention mechanism to combine outputs\n",
    "    attention = Attention()([lstm_output, tcn_output])\n",
    "    merged = Concatenate()([lstm_output, tcn_output, attention])\n",
    "\n",
    "    # Add a dense layer for the final prediction\n",
    "    merged = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    # Create the ensemble model\n",
    "    ensemble_model = Model(inputs=[lstm_input, tcn_input], outputs=merged)\n",
    "\n",
    "    # Compile the model\n",
    "    ensemble_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "    return ensemble_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new data for the hybrid model\n",
    "def predict_future_failures_hybrid(model, input_data_lstm, input_data_tcn, sequence_length, prediction_steps):\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(prediction_steps):\n",
    "        # Make predictions for the next time step using both LSTM and TCN models\n",
    "        prediction = model.predict([input_data_lstm.reshape(1, sequence_length, input_data_lstm.shape[1]),\n",
    "                                    input_data_tcn.reshape(1, sequence_length, input_data_tcn.shape[1])])\n",
    "        predictions.append(prediction[0, 0])\n",
    "\n",
    "        # Shift the input data by one time step and append the new prediction\n",
    "        input_data_lstm = np.roll(input_data_lstm, shift=-1, axis=0)\n",
    "        input_data_lstm[-1, -1] = prediction[0, 0]\n",
    "\n",
    "        input_data_tcn = np.roll(input_data_tcn, shift=-1, axis=0)\n",
    "        input_data_tcn[-1, -1] = prediction[0, 0]\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 days -> 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_selected = df[['timestamp', 'state'] + selected_features].copy()\n",
    "\n",
    "# Encode the target variable 'state' to binary (0 for \"COMPLETED\", 1 otherwise)\n",
    "df_selected['target'] = (df_selected['state'] != 'COMPLETED').astype(int)\n",
    "\n",
    "# Drop the original 'state' column\n",
    "df_selected.drop('state', axis=1, inplace=True)\n",
    "\n",
    "# Normalize selected features\n",
    "scaler = MinMaxScaler()\n",
    "df_selected[selected_features] = scaler.fit_transform(df_selected[selected_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of target variable: -0.7362924419461765\n",
      "Maximum value of target variable: 5.322951443655186\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 3s 196ms/step - loss: 0.7782 - mean_squared_error: 0.7782 - val_loss: 0.9101 - val_mean_squared_error: 0.9101\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.7660 - mean_squared_error: 0.7660 - val_loss: 0.8969 - val_mean_squared_error: 0.8969\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.7728 - mean_squared_error: 0.7728 - val_loss: 0.8794 - val_mean_squared_error: 0.8794\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.7787 - mean_squared_error: 0.7787 - val_loss: 0.8702 - val_mean_squared_error: 0.8702\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.7784 - mean_squared_error: 0.7784 - val_loss: 0.8622 - val_mean_squared_error: 0.8622\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.7753 - mean_squared_error: 0.7753 - val_loss: 0.8593 - val_mean_squared_error: 0.8593\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.7644 - mean_squared_error: 0.7644 - val_loss: 0.8542 - val_mean_squared_error: 0.8542\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.7641 - mean_squared_error: 0.7641 - val_loss: 0.8488 - val_mean_squared_error: 0.8488\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.7643 - mean_squared_error: 0.7643 - val_loss: 0.8459 - val_mean_squared_error: 0.8459\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.7655 - mean_squared_error: 0.7655 - val_loss: 0.8422 - val_mean_squared_error: 0.8422\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - 3s 490ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - 6s 836ms/step - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 1.1630 - val_mean_squared_error: 1.1630\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.0186 - mean_squared_error: 1.0186 - val_loss: 1.1586 - val_mean_squared_error: 1.1586\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.0160 - mean_squared_error: 1.0160 - val_loss: 1.1544 - val_mean_squared_error: 1.1544\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.1475 - val_mean_squared_error: 1.1475\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 1.1404 - val_mean_squared_error: 1.1404\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 1.1348 - val_mean_squared_error: 1.1348\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.0029 - mean_squared_error: 1.0029 - val_loss: 1.1282 - val_mean_squared_error: 1.1282\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.0013 - mean_squared_error: 1.0013 - val_loss: 1.1218 - val_mean_squared_error: 1.1218\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.9989 - mean_squared_error: 0.9989 - val_loss: 1.1145 - val_mean_squared_error: 1.1145\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.9966 - mean_squared_error: 0.9966 - val_loss: 1.1091 - val_mean_squared_error: 1.1091\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.9937 - mean_squared_error: 0.9937 - val_loss: 1.1029 - val_mean_squared_error: 1.1029\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.9907 - mean_squared_error: 0.9907 - val_loss: 1.0936 - val_mean_squared_error: 1.0936\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.9880 - mean_squared_error: 0.9880 - val_loss: 1.0873 - val_mean_squared_error: 1.0873\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.9826 - mean_squared_error: 0.9826 - val_loss: 1.0770 - val_mean_squared_error: 1.0770\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.9756 - mean_squared_error: 0.9756 - val_loss: 1.0714 - val_mean_squared_error: 1.0714\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.9729 - mean_squared_error: 0.9729 - val_loss: 1.0663 - val_mean_squared_error: 1.0663\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.9699 - mean_squared_error: 0.9699 - val_loss: 1.0625 - val_mean_squared_error: 1.0625\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.9656 - mean_squared_error: 0.9656 - val_loss: 1.0584 - val_mean_squared_error: 1.0584\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.9637 - mean_squared_error: 0.9637 - val_loss: 1.0525 - val_mean_squared_error: 1.0525\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.9587 - mean_squared_error: 0.9587 - val_loss: 1.0494 - val_mean_squared_error: 1.0494\n"
     ]
    }
   ],
   "source": [
    "# Prepare data with daily intervals\n",
    "data_day = prepare_data(df_selected, time_intervals['day'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_day, targets_day = create_sequences(data_day, sequence_length)\n",
    "\n",
    "# Define the index to split the data\n",
    "split_index_day = int(len(sequences_day) * 0.7)  # Use 70% of the data for training\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_day, X_test_day = sequences_day[:split_index_day], sequences_day[split_index_day:]\n",
    "y_train_day, y_test_day = targets_day[:split_index_day], targets_day[split_index_day:]\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model_day = Sequential()\n",
    "lstm_model_day.add(LSTM(50, input_shape=(X_train_day.shape[1], X_train_day.shape[2])))\n",
    "lstm_model_day.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_day.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model_day.fit(X_train_day, y_train_day, epochs=10, batch_size=16, validation_split=0.15)\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model_day = Sequential([\n",
    "    TCN(input_shape=(sequence_length, X_train_day.shape[2])),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "tcn_model_day.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "tcn_model_day.fit(X_train_day, y_train_day, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Create the hybrid model\n",
    "hybrid_model_attention_day = create_attention_hybrid_model(lstm_model_day, tcn_model_day)\n",
    "\n",
    "# Train the hybrid model with both LSTM and TCN data\n",
    "history_day = hybrid_model_attention_day.fit([X_train_day, X_train_day], y_train_day, epochs=20, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance_Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.0995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squared Error\n",
    "mse_day = hybrid_model_attention_day.evaluate([X_test_day, X_test_day], y_test_day, verbose=0)[1]\n",
    "print(f'Mean Squared Error: {mse_day:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate model on test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_pred_day \u001b[38;5;241m=\u001b[39m \u001b[43mhybrid_model_attention_day\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_day\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:272\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m _check_data_cardinality(inputs)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# If batch_size is not passed but steps is, calculate from the input\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# data.  Defaults to `32` for backwards compatibility.\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_size:\n\u001b[1;32m    273\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mceil(num_samples \u001b[38;5;241m/\u001b[39m steps)) \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mceil(num_samples \u001b[38;5;241m/\u001b[39m batch_size))\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test data\n",
    "y_pred_day = hybrid_model_attention_day.predict(X_test_day, X_test_day)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Root Mean Squared Error\n",
    "rmse_day = mean_squared_error(y_test_day, y_pred_day, squared=False)\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_day:.4f}\\n')\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "mae_day = mean_absolute_error(y_test_day, y_pred_day)\n",
    "print(f'Mean Abosolute Error: {mae_day:.4f}\\n')\n",
    "\n",
    "# Calculate R-squared\n",
    "r2_day = r2_score(y_test_day, y_pred_day)\n",
    "print(f'R-squared (R2): {r2_day:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(history_day.history['loss'], label='Training Loss')\n",
    "plt.plot(history_day.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Failures vs. True Failures Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_min_day = -0.7362924419461765\n",
    "target_max_day = 5.322951443655186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 2.3448\n",
      "\n",
      "1/1 [==============================] - 1s 747ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted Failures for the Next 7 Time Steps:\n",
      "[0.5312288, 0.49342206, 0.49473184, 0.5320658, 0.53384113, 0.5329574, 0.5334162]\n",
      "\n",
      "Mean Squared Error for Predictions: 4.6117\n",
      "\n",
      "Mean Absolute Error for Predictions: 1.7128\n"
     ]
    }
   ],
   "source": [
    "index_day = 0  # Can be any valid starting point\n",
    "\n",
    "input_data_day = X_test_day[index_day]\n",
    "\n",
    "# Make predictions\n",
    "predicted_failures_day = predict_future_failures_hybrid(lstm_model_day, input_data_day, sequence_length, prediction_steps)\n",
    "\n",
    "# # Denormalize the predicted failures \n",
    "# predicted_failures_denormalized_day = np.array(predicted_failures_day) * (target_max_day - target_min_day) + target_min_day\n",
    "\n",
    "# Get the true failures for the specified number of days\n",
    "true_failures_day = y_test_day[index_day:index_day + prediction_steps]\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted failures for the next 7 days:\")\n",
    "print(predicted_failures_day)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_day - true_failures_day)**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions = np.mean(np.abs(predicted_failures_day - true_failures_day))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions:.4f}')\n",
    "\n",
    "# Plot predicted failures vs. true failures\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(predicted_failures_day, label='Predicted Failures', marker='o')\n",
    "plt.plot(true_failures_day, label='True Failures', marker='x')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number of Failures')\n",
    "plt.title('Predicted Failures vs. True Failures')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 hours -> 7 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_selected = df[['timestamp', 'state'] + selected_features].copy()\n",
    "\n",
    "# Encode the target variable 'state' to binary (0 for \"COMPLETED\", 1 otherwise)\n",
    "df_selected['target'] = (df_selected['state'] != 'COMPLETED').astype(int)\n",
    "\n",
    "# Drop the original 'state' column\n",
    "df_selected.drop('state', axis=1, inplace=True)\n",
    "\n",
    "# Normalize selected features\n",
    "scaler = MinMaxScaler()\n",
    "df_selected[selected_features] = scaler.fit_transform(df_selected[selected_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['timestamp'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_723728/4148490983.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Prepare data with hourly intervals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_hour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_intervals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create sequences and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msequences_hour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_hour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_hour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_723728/1004066937.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data, time_interval)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdata_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtarget_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   5869\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5870\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5873\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5876\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['timestamp'] are in the columns\""
     ]
    }
   ],
   "source": [
    "# Prepare data with hourly intervals\n",
    "data_hour = prepare_data(df_selected, time_intervals['hour'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_hour, targets_hour = create_sequences(data_hour, sequence_length)\n",
    "\n",
    "# Define the index to split the data\n",
    "split_index_hour = int(len(sequences_hour) * 0.7)  # Use 70% of the data for training\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_hour, X_test_hour = sequences_hour[:split_index_hour], sequences_hour[split_index_hour:]\n",
    "y_train_hour, y_test_hour = targets_hour[:split_index_hour], targets_hour[split_index_hour:]\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model_hour = Sequential()\n",
    "lstm_model_hour.add(LSTM(50, input_shape=(X_train_hour.shape[1], X_train_hour.shape[2])))\n",
    "lstm_model_hour.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_hour.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model_hour.fit(X_train_hour, y_train_hour, epochs=10, batch_size=16, validation_split=0.15)\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model_hour = Sequential([\n",
    "    TCN(input_shape=(sequence_length, X_train_hour.shape[2])),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "tcn_model_hour.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "tcn_model_hour.fit(X_train_hour, y_train_hour, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Create the hybrid model\n",
    "hybrid_model_hour_attention = create_attention_hybrid_model(lstm_model_hour, tcn_model_hour)\n",
    "\n",
    "# Train the hybrid model with both LSTM and TCN data\n",
    "hybrid_model_hour_attention.fit([X_train_hour, X_train_hour], y_train_hour, epochs=20, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance_Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using Mean Squared Error\n",
    "mse_hour = lstm_model_hour.evaluate(X_test_hour, y_test_hour, verbose=0)[1]\n",
    "print(f'Mean Squared Error: {mse_hour:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "y_pred_hour = lstm_model_hour.predict(X_test_hour)\n",
    "\n",
    "# Calculate Root Mean Squared Error\n",
    "rmse_hour = mean_squared_error(y_test_hour, y_pred_hour, squared=False)\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_hour:.4f}\\n')\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "mae_hour = mean_absolute_error(y_test_hour, y_pred_hour)\n",
    "print(f'Mean Abosolute Error: {mae_hour:.4f}\\n')\n",
    "\n",
    "# Calculate R-squared\n",
    "r2_hour = r2_score(y_test_hour, y_pred_hour)\n",
    "print(f'R-squared (R2): {r2_hour:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 0.1999\n",
      "\n",
      "1/1 [==============================] - 1s 797ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted Failures for the Next 7 Time Steps (Hybrid):\n",
      "[0.4002875, 0.4002881, 0.4002926, 0.4003259, 0.40057176, 0.4023942, 0.41613513]\n",
      "\n",
      "Mean Squared Error for Predictions: 0.2193\n",
      "\n",
      "Mean Absolute Error for Predictions: 0.4598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squared Error\n",
    "mse_hour_attention = hybrid_model_hour_attention.evaluate([X_test_hour, X_test_hour], y_test_hour, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mse_hour_attention:.4f}\\n')\n",
    "\n",
    "# Make predictions with the hybrid model\n",
    "predicted_failures_hybrid_attention = predict_future_failures_hybrid(hybrid_model_hour_attention, input_data_lstm, input_data_tcn, sequence_length, prediction_steps_hybrid)\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted Failures for the Next 7 Time Steps (Hybrid):\")\n",
    "print(predicted_failures_hybrid_attention)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_hybrid_attention - y_test_hour[3:3+prediction_steps])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions_hybrid_attention_hour = np.mean(np.abs(predicted_failures_hybrid_attention - y_test_hour[3:3+prediction_steps]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions_hybrid_attention_hour:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7033/7033 [==============================] - 153s 21ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 2/10\n",
      "7033/7033 [==============================] - 150s 21ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 3/10\n",
      "7033/7033 [==============================] - 137s 19ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 4/10\n",
      "7033/7033 [==============================] - 127s 18ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 5/10\n",
      "7033/7033 [==============================] - 132s 19ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 6/10\n",
      "7033/7033 [==============================] - 120s 17ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 7/10\n",
      "7033/7033 [==============================] - 114s 16ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 8/10\n",
      "7033/7033 [==============================] - 121s 17ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 9/10\n",
      "7033/7033 [==============================] - 130s 18ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 10/10\n",
      "7033/7033 [==============================] - 131s 19ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 1/20\n",
      "3723/3723 [==============================] - 142s 37ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 2/20\n",
      "3723/3723 [==============================] - 106s 29ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 3/20\n",
      "3723/3723 [==============================] - 117s 31ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 4/20\n",
      "3723/3723 [==============================] - 123s 33ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 5/20\n",
      "3723/3723 [==============================] - 116s 31ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 6/20\n",
      "3723/3723 [==============================] - 122s 33ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 7/20\n",
      "3723/3723 [==============================] - 127s 34ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 8/20\n",
      "3723/3723 [==============================] - 126s 34ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 9/20\n",
      "3723/3723 [==============================] - 102s 27ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 10/20\n",
      "3723/3723 [==============================] - 118s 32ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 11/20\n",
      "3723/3723 [==============================] - 119s 32ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 12/20\n",
      "3723/3723 [==============================] - 128s 34ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 13/20\n",
      "3723/3723 [==============================] - 111s 30ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 14/20\n",
      "3723/3723 [==============================] - 117s 31ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 15/20\n",
      "3723/3723 [==============================] - 148s 40ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 16/20\n",
      "3723/3723 [==============================] - 140s 38ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 17/20\n",
      "3723/3723 [==============================] - 131s 35ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 18/20\n",
      "3723/3723 [==============================] - 126s 34ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 19/20\n",
      "3723/3723 [==============================] - 138s 37ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 20/20\n",
      "3723/3723 [==============================] - 149s 40ms/step - loss: 0.4188 - mean_squared_error: 0.4188 - val_loss: 0.4171 - val_mean_squared_error: 0.4171\n",
      "Epoch 1/10\n",
      "3723/3723 [==============================] - 161s 42ms/step - loss: 0.2442 - mean_squared_error: 0.2442 - val_loss: 0.2431 - val_mean_squared_error: 0.2431\n",
      "Epoch 2/10\n",
      "3723/3723 [==============================] - 136s 37ms/step - loss: 0.1984 - mean_squared_error: 0.1984 - val_loss: 0.0773 - val_mean_squared_error: 0.0773\n",
      "Epoch 3/10\n",
      "3723/3723 [==============================] - 132s 35ms/step - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0131 - val_mean_squared_error: 0.0131\n",
      "Epoch 4/10\n",
      "3723/3723 [==============================] - 135s 36ms/step - loss: 0.0071 - mean_squared_error: 0.0071 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 5/10\n",
      "3723/3723 [==============================] - 136s 36ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 6/10\n",
      "3723/3723 [==============================] - 138s 37ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 7/10\n",
      "3723/3723 [==============================] - 136s 36ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 8/10\n",
      "3723/3723 [==============================] - 129s 35ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 9/10\n",
      "3723/3723 [==============================] - 119s 32ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 10/10\n",
      "3723/3723 [==============================] - 128s 34ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb1960b9290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data\n",
    "data_minute = prepare_data(df_selected, time_intervals['minute'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_minute, targets_minute = create_sequences(data_minute, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_minute, X_test_minute, y_train_minute, y_test_minute = train_test_split(sequences_minute, targets_minute, test_size=0.2, random_state=1)\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model_minute = Sequential()\n",
    "lstm_model_minute.add(LSTM(50, input_shape=(X_train_minute.shape[1], X_train_minute.shape[2])))\n",
    "lstm_model_minute.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_minute.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model_minute.fit(X_train_minute, y_train_minute, epochs=10, batch_size=16, validation_split=0.15)\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model_minute = Sequential([\n",
    "    TCN(input_shape=(sequence_length, X_train_minute.shape[2])),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tcn_model_minute.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "tcn_model_minute.fit(X_train_minute, y_train_minute, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Select a starting point for predictions\n",
    "input_data_lstm_hybrid = X_test_minute[3]\n",
    "input_data_tcn_hybrid = X_test_minute[3]\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps = 7\n",
    "\n",
    "# Create the hybrid model\n",
    "hybrid_model_minute_attention = create_attention_hybrid_model(lstm_model_minute, tcn_model_minute)\n",
    "\n",
    "# Train the hybrid model with both LSTM and TCN data\n",
    "hybrid_model_minute_attention.fit([X_train_minute, X_train_minute], y_train_minute, epochs=10, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 0.0013\n",
      "\n",
      "1/1 [==============================] - 1s 811ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted Failures for the Next 7 Time Steps (Hybrid):\n",
      "[0.003039969, 0.0030414464, 0.0030468209, 0.0030486726, 0.0030448635, 0.0030444176, 0.003047832]\n",
      "\n",
      "Mean Squared Error for Predictions: 0.4260\n",
      "\n",
      "Mean Absolute Error for Predictions: 0.4290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squared Error\n",
    "mse_minute_attention = hybrid_model_minute_attention.evaluate([X_test_minute, X_test_minute], y_test_minute, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mse_minute_attention:.4f}\\n')\n",
    "\n",
    "# Make predictions with the hybrid model\n",
    "predicted_failures_hybrid_attention = predict_future_failures_hybrid(hybrid_model_minute_attention, input_data_lstm_hybrid, input_data_tcn_hybrid, sequence_length, prediction_steps)\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted Failures for the Next 7 Time Steps (Hybrid):\")\n",
    "print(predicted_failures_hybrid_attention)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_hybrid_attention - y_test_minute[3:3+prediction_steps])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions_hybrid_attention_minute = np.mean(np.abs(predicted_failures_hybrid_attention - y_test_minute[3:3+prediction_steps]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions_hybrid_attention_minute:.4f}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
