{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tcn import TCN\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Concatenate, Attention, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"myData2.parquet\"\n",
    "df = pd.read_parquet(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['timestamp_seconds', \n",
    "                     'node_memory_Percpu_bytes', \n",
    "                     'node_context_switches_total', \n",
    "                     'surfsara_power_usage', \n",
    "                     'node_netstat_Tcp_InSegs', \n",
    "                     'node_netstat_Tcp_OutSegs', \n",
    "                     'node_network_transmit_packets_total-sum', \n",
    "                     'node_filesystem_size_bytes-sum', \n",
    "                     'node_filesystem_files-sum', \n",
    "                     'node_memory_MemFree_bytes', \n",
    "                     'node_netstat_Tcp_InErrs']\n",
    "# FixMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_selected = df[['timestamp', 'state'] + selected_features].copy()\n",
    "\n",
    "# Encode the target variable 'state' to binary (0 for \"COMPLETED\", 1 otherwise)\n",
    "df_selected['target'] = (df_selected['state'] != 'COMPLETED').astype(int)\n",
    "\n",
    "# Drop the original 'state' column\n",
    "df_selected.drop('state', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time intervals\n",
    "time_intervals = {'minute': '1T', 'hour': '1H', 'day': '1D'}\n",
    "\n",
    "# Normalize selected features\n",
    "scaler = MinMaxScaler()\n",
    "df_selected[selected_features] = scaler.fit_transform(df_selected[selected_features])\n",
    "\n",
    "# Set sequence length\n",
    "sequence_length = 30\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps_hybrid = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data\n",
    "def prepare_data(data, time_interval):\n",
    "    data.set_index('timestamp', inplace=True)\n",
    "    data_resampled = data.resample(time_interval).sum()\n",
    "    data_resampled['target'] = data_resampled['target'].clip(upper=1)  # Clip values to 1\n",
    "\n",
    "    target_min = data_resampled['target'].min()\n",
    "    target_max = data_resampled['target'].max()\n",
    "    print(\"Minimum value of target variable:\", target_min)\n",
    "    print(\"Maximum value of target variable:\", target_max)\n",
    "\n",
    "    return data_resampled\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length].values\n",
    "        target = data.iloc[i+sequence_length]['target']\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a hybrid model with attention mechanism\n",
    "def create_attention_hybrid_model(lstm_model, tcn_model):\n",
    "    lstm_input = lstm_model.input\n",
    "    tcn_input = tcn_model.input\n",
    "\n",
    "    # Get the output layers of both models\n",
    "    lstm_output = lstm_model.layers[-1].output\n",
    "    tcn_output = tcn_model.layers[-1].output\n",
    "\n",
    "    # Use Attention mechanism to combine outputs\n",
    "    attention = Attention()([lstm_output, tcn_output])\n",
    "    merged = Concatenate()([lstm_output, tcn_output, attention])\n",
    "\n",
    "    # Add a dense layer for the final prediction\n",
    "    merged = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    # Create the ensemble model\n",
    "    ensemble_model = Model(inputs=[lstm_input, tcn_input], outputs=merged)\n",
    "\n",
    "    # Compile the model\n",
    "    ensemble_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "    return ensemble_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new data for the hybrid model\n",
    "def predict_future_failures_hybrid(model, input_data_lstm, input_data_tcn, sequence_length, prediction_steps):\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(prediction_steps):\n",
    "        # Make predictions for the next time step using both LSTM and TCN models\n",
    "        prediction = model.predict([input_data_lstm.reshape(1, sequence_length, input_data_lstm.shape[1]),\n",
    "                                    input_data_tcn.reshape(1, sequence_length, input_data_tcn.shape[1])])\n",
    "        predictions.append(prediction[0, 0])\n",
    "\n",
    "        # Shift the input data by one time step and append the new prediction\n",
    "        input_data_lstm = np.roll(input_data_lstm, shift=-1, axis=0)\n",
    "        input_data_lstm[-1, -1] = prediction[0, 0]\n",
    "\n",
    "        input_data_tcn = np.roll(input_data_tcn, shift=-1, axis=0)\n",
    "        input_data_tcn[-1, -1] = prediction[0, 0]\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of target variable: 0\n",
      "Maximum value of target variable: 1\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 2s 172ms/step - loss: 0.2727 - mean_squared_error: 0.2727 - val_loss: 0.2793 - val_mean_squared_error: 0.2793\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2402 - mean_squared_error: 0.2402 - val_loss: 0.2526 - val_mean_squared_error: 0.2526\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2329 - mean_squared_error: 0.2329 - val_loss: 0.2002 - val_mean_squared_error: 0.2002\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2208 - mean_squared_error: 0.2208 - val_loss: 0.1860 - val_mean_squared_error: 0.1860\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2126 - mean_squared_error: 0.2126 - val_loss: 0.1760 - val_mean_squared_error: 0.1760\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.1970 - mean_squared_error: 0.1970 - val_loss: 0.1685 - val_mean_squared_error: 0.1685\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.1959 - mean_squared_error: 0.1959 - val_loss: 0.1715 - val_mean_squared_error: 0.1715\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.1896 - mean_squared_error: 0.1896 - val_loss: 0.1546 - val_mean_squared_error: 0.1546\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.1833 - mean_squared_error: 0.1833 - val_loss: 0.1578 - val_mean_squared_error: 0.1578\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.1817 - mean_squared_error: 0.1817 - val_loss: 0.1579 - val_mean_squared_error: 0.1579\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - 3s 456ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.8571 - val_mean_squared_error: 0.8571\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - 5s 829ms/step - loss: 0.2234 - mean_squared_error: 0.2234 - val_loss: 0.1999 - val_mean_squared_error: 0.1999\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.2227 - mean_squared_error: 0.2227 - val_loss: 0.1989 - val_mean_squared_error: 0.1989\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2220 - mean_squared_error: 0.2220 - val_loss: 0.1981 - val_mean_squared_error: 0.1981\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.2212 - mean_squared_error: 0.2212 - val_loss: 0.1977 - val_mean_squared_error: 0.1977\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.2207 - mean_squared_error: 0.2207 - val_loss: 0.1965 - val_mean_squared_error: 0.1965\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.2202 - mean_squared_error: 0.2202 - val_loss: 0.1958 - val_mean_squared_error: 0.1958\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2197 - mean_squared_error: 0.2197 - val_loss: 0.1948 - val_mean_squared_error: 0.1948\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.2188 - mean_squared_error: 0.2188 - val_loss: 0.1933 - val_mean_squared_error: 0.1933\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2183 - mean_squared_error: 0.2183 - val_loss: 0.1922 - val_mean_squared_error: 0.1922\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.2178 - mean_squared_error: 0.2178 - val_loss: 0.1915 - val_mean_squared_error: 0.1915\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2175 - mean_squared_error: 0.2175 - val_loss: 0.1908 - val_mean_squared_error: 0.1908\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2170 - mean_squared_error: 0.2170 - val_loss: 0.1895 - val_mean_squared_error: 0.1895\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2168 - mean_squared_error: 0.2168 - val_loss: 0.1889 - val_mean_squared_error: 0.1889\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2166 - mean_squared_error: 0.2166 - val_loss: 0.1882 - val_mean_squared_error: 0.1882\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2162 - mean_squared_error: 0.2162 - val_loss: 0.1877 - val_mean_squared_error: 0.1877\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2159 - mean_squared_error: 0.2159 - val_loss: 0.1872 - val_mean_squared_error: 0.1872\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2156 - mean_squared_error: 0.2156 - val_loss: 0.1867 - val_mean_squared_error: 0.1867\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2153 - mean_squared_error: 0.2153 - val_loss: 0.1864 - val_mean_squared_error: 0.1864\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2152 - mean_squared_error: 0.2152 - val_loss: 0.1857 - val_mean_squared_error: 0.1857\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2151 - mean_squared_error: 0.2151 - val_loss: 0.1849 - val_mean_squared_error: 0.1849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f0408d76490>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data with daily intervals\n",
    "data_day = prepare_data(df_selected, time_intervals['day'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_day, targets_day = create_sequences(data_day, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_day, X_test_day, y_train_day, y_test_day = train_test_split(sequences_day, targets_day, test_size=0.2, random_state=1)\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model_day = Sequential()\n",
    "lstm_model_day.add(LSTM(50, input_shape=(X_train_day.shape[1], X_train_day.shape[2])))\n",
    "lstm_model_day.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_day.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model_day.fit(X_train_day, y_train_day, epochs=10, batch_size=16, validation_split=0.15)\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model_day = Sequential([\n",
    "    TCN(input_shape=(sequence_length, X_train_day.shape[2])),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tcn_model_day.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "tcn_model_day.fit(X_train_day, y_train_day, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Create the hybrid model\n",
    "hybrid_model_day_attention = create_attention_hybrid_model(lstm_model_day, tcn_model_day)\n",
    "\n",
    "# Train the hybrid model with both LSTM and TCN data\n",
    "hybrid_model_day_attention.fit([X_train_day, X_train_day], y_train_day, epochs=20, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mean Squared Error: 0.1820\n",
      "\n",
      "1/1 [==============================] - 1s 670ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted Failures for the Next 7 Time Steps:\n",
      "[0.59147006, 0.6053148, 0.60256386, 0.60772824, 0.5980923, 0.598887, 0.60436183]\n",
      "\n",
      "Mean Squared Error for Predictions: 0.1591\n",
      "\n",
      "Mean Absolute Error for Predictions: 0.3988\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Squared Error\n",
    "mse_day_attention = hybrid_model_day_attention.evaluate([X_test_day, X_test_day], y_test_day, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mse_day_attention:.4f}\\n')\n",
    "\n",
    "# Select a starting point for predictions\n",
    "input_data_lstm_hybrid = X_test_day[3]\n",
    "input_data_tcn_hybrid = X_test_day[3]\n",
    "\n",
    "# Make predictions with the hybrid model\n",
    "predicted_failures_hybrid_attention = predict_future_failures_hybrid(hybrid_model_day_attention, input_data_lstm_hybrid, input_data_tcn_hybrid, sequence_length, prediction_steps_hybrid)\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted Failures for the Next 7 Time Steps:\")\n",
    "print(predicted_failures_hybrid_attention)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_hybrid_attention - y_test_day[3:3+prediction_steps_hybrid])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions_hybrid_attention_day = np.mean(np.abs(predicted_failures_hybrid_attention - y_test_day[3:3+prediction_steps_hybrid]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions_hybrid_attention_day:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['timestamp'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_726994/4148490983.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Prepare data with hourly intervals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_hour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_intervals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create sequences and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msequences_hour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_hour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_hour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_726994/784687535.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data, time_interval)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdata_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clip values to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   5869\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5870\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5873\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5876\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['timestamp'] are in the columns\""
     ]
    }
   ],
   "source": [
    "# Prepare data with hourly intervals\n",
    "data_hour = prepare_data(df_selected, time_intervals['hour'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_hour, targets_hour = create_sequences(data_hour, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_hour, X_test_hour, y_train_hour, y_test_hour = train_test_split(sequences_hour, targets_hour, test_size=0.2, random_state=1)\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model_hour = Sequential()\n",
    "lstm_model_hour.add(LSTM(50, input_shape=(X_train_hour.shape[1], X_train_hour.shape[2])))\n",
    "lstm_model_hour.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_hour.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model_hour.fit(X_train_hour, y_train_hour, epochs=10, batch_size=16, validation_split=0.15)\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model_hour = Sequential([\n",
    "    TCN(input_shape=(sequence_length, X_train_hour.shape[2])),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tcn_model_hour.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "tcn_model_hour.fit(X_train_hour, y_train_hour, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Select a starting point for predictions\n",
    "input_data_lstm_hybrid = X_test_hour[3]\n",
    "input_data_tcn_hybrid = X_test_hour[3]\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps_hybrid = 7\n",
    "\n",
    "# Create the hybrid model\n",
    "hybrid_model_hour_attention = create_attention_hybrid_model(lstm_model_hour, tcn_model_hour)\n",
    "\n",
    "# Train the hybrid model with both LSTM and TCN data\n",
    "hybrid_model_hour_attention.fit([X_train_hour, X_train_hour], y_train_hour, epochs=20, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using Mean Squared Error\n",
    "mse_hour_attention = hybrid_model_hour_attention.evaluate([X_test_hour, X_test_hour], y_test_hour, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mse_hour_attention:.4f}\\n')\n",
    "\n",
    "# Make predictions with the hybrid model\n",
    "predicted_failures_hybrid_attention = predict_future_failures_hybrid(hybrid_model_hour_attention, input_data_lstm_hybrid, input_data_tcn_hybrid, sequence_length, prediction_steps_hybrid)\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted Failures for the Next 7 Time Steps (Hybrid):\")\n",
    "print(predicted_failures_hybrid_attention)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_hybrid_attention - y_test_hour[3:3+prediction_steps_hybrid])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions_hybrid_attention_hour = np.mean(np.abs(predicted_failures_hybrid_attention - y_test_hour[3:3+prediction_steps_hybrid]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions_hybrid_attention_hour:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data_minute = prepare_data(df_selected, time_intervals['minute'])\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences_minute, targets_minute = create_sequences(data_minute, sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_minute, X_test_minute, y_train_minute, y_test_minute = train_test_split(sequences_minute, targets_minute, test_size=0.2, random_state=1)\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model_minute = Sequential()\n",
    "lstm_model_minute.add(LSTM(50, input_shape=(X_train_minute.shape[1], X_train_minute.shape[2])))\n",
    "lstm_model_minute.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_minute.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model_minute.fit(X_train_minute, y_train_minute, epochs=10, batch_size=16, validation_split=0.15)\n",
    "\n",
    "# Build the TCN model\n",
    "tcn_model_minute = Sequential([\n",
    "    TCN(input_shape=(sequence_length, X_train_minute.shape[2])),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tcn_model_minute.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "tcn_model_minute.fit(X_train_minute, y_train_minute, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Select a starting point for predictions\n",
    "input_data_lstm_hybrid = X_test_minute[3]\n",
    "input_data_tcn_hybrid = X_test_minute[3]\n",
    "\n",
    "# Number of time steps to predict into the future\n",
    "prediction_steps_hybrid = 7\n",
    "\n",
    "# Create the hybrid model\n",
    "hybrid_model_minute_attention = create_attention_hybrid_model(lstm_model_minute, tcn_model_minute)\n",
    "\n",
    "# Train the hybrid model with both LSTM and TCN data\n",
    "hybrid_model_minute_attention.fit([X_train_minute, X_train_minute], y_train_minute, epochs=10, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using Mean Squared Error\n",
    "mse_minute_attention = hybrid_model_minute_attention.evaluate([X_test_minute, X_test_minute], y_test_minute, verbose=0)[1]\n",
    "print(f'Model Mean Squared Error: {mse_minute_attention:.4f}\\n')\n",
    "\n",
    "# Make predictions with the hybrid model\n",
    "predicted_failures_hybrid_attention = predict_future_failures_hybrid(hybrid_model_minute_attention, input_data_lstm_hybrid, input_data_tcn_hybrid, sequence_length, prediction_steps_hybrid)\n",
    "\n",
    "# Print the predicted failures\n",
    "print(\"Predicted Failures for the Next 7 Time Steps (Hybrid):\")\n",
    "print(predicted_failures_hybrid_attention)\n",
    "\n",
    "# Evaluate the predictions using Mean Squared Error\n",
    "mse_predictions = np.mean((predicted_failures_hybrid_attention - y_test_minute[3:3+prediction_steps_hybrid])**2)\n",
    "print(f'\\nMean Squared Error for Predictions: {mse_predictions:.4f}\\n')\n",
    "\n",
    "# Evaluate the predictions using Mean Absolute Error\n",
    "mae_predictions_hybrid_attention_minute = np.mean(np.abs(predicted_failures_hybrid_attention - y_test_minute[3:3+prediction_steps_hybrid]))\n",
    "print(f'Mean Absolute Error for Predictions: {mae_predictions_hybrid_attention_minute:.4f}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
