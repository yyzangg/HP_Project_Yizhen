{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required preprocessing/parsing of the job data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import time, datetime, pytz\n",
    "from matplotlib.ticker import MultipleLocator, FixedLocator, LogLocator, NullFormatter, ScalarFormatter\n",
    "from datetime import date, datetime, time, timedelta\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess raw slurmdata collected from SLURM:\n",
    "#   convert data tpye -> int \n",
    "#   Unknown -> NaN values\n",
    "\n",
    "def preprocess_df(file_name):\n",
    "    with open(os.path.join(file_path, file_name),'r') as file:\n",
    "        filedata = file.read()\n",
    "        filedata = filedata.replace('None assigned','NoneAssigned')\n",
    "\n",
    "    data = pd.read_fwf(os.path.join(file_path, file_name), delimiter = r\"\\s\", header = None)\n",
    "    data = data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "    data = data.rename(columns=data.iloc[0]).drop(data.index[0])\n",
    "    data = data.iloc[1:]\n",
    "    data = data.astype({\"ElapsedRaw\": int, \"CPUTimeRAW\": int, \"NCPUS\": int, \"NNode\":int, \"AllocCPUS\":int, \"AllocNode\":int, \"ReqCPUS\":int})\n",
    "    data.replace(to_replace = r'^\\s*$', value = np.nan, regex = True, inplace = True)  # Blank value->NaN value\n",
    "    data.replace(to_replace = r'Unknown', value = np.nan, regex = True, inplace = True)  # Unknown value->NaN value\n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/yzang/Documents/Slurm data/Raw data/\"\n",
    "data_0 = preprocess_df(\"slurm_data_0322-0223.csv\")\n",
    "data_0.to_csv(file_path + \"raw_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data of Jan 2022 and Feb 2022\n",
    "slurm_0122 = preprocess_df(\"slurm_data_0122.csv\")\n",
    "slurm_0222 = preprocess_df(\"slurm_data_0222.csv\")\n",
    "slurm_0322 = preprocess_df(\"slurm_data_0322.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data of Jan 2022 and Feb 2022\n",
    "data_1 = pd.merge(slurm_0122, slurm_0222, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = pd.merge(data_1, slurm_0322, how = \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/ckwc3nwj0cjbxyphnn5k3nl40000gp/T/ipykernel_21676/923379924.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data_2.append(data_0)\n"
     ]
    }
   ],
   "source": [
    "# Merge the two datasets\n",
    "data = data_2.append(data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values in Start, End\n",
    "data = data.dropna(subset = [\"Start\", \"End\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify the \"State\" metrics\n",
    "data.State = data.State.apply(lambda x: \"CANCELLED\" if 'CANCELLED' in x else x)\n",
    "data.State = data.State.apply(lambda x: \"OUT_OF_MEMORY\" if 'OUT_OF_ME' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMPLETED        1768608\n",
       "FAILED            424160\n",
       "CANCELLED         152336\n",
       "TIMEOUT            67168\n",
       "OUT_OF_MEMORY      13390\n",
       "NODE_FAIL             56\n",
       "REQUEUED               6\n",
       "Name: State, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.State.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parses node strings like r12n[1-30,32] to r12n1, r12n2 ... r12n30, r12n32\n",
    "def split_nodes(s):\n",
    "    if s is None or len(s) == 0:\n",
    "        return set()\n",
    "    \n",
    "    s = s.replace(\",+\", \"\").replace(\"+\", \"\").replace(\"\\r\\n\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "\n",
    "    start = 0\n",
    "    index = 0\n",
    "    rack_chunks = []\n",
    "    in_bracket = False\n",
    "    while index < len(s):  # Separate them in parts like r12n[1-30,32] or r13n1\n",
    "        if s[index] == \"[\":\n",
    "            in_bracket = True\n",
    "        elif s[index] == \"]\":\n",
    "            in_bracket = False\n",
    "        elif s[index] == \",\" and not in_bracket:\n",
    "            rack_chunks.append(s[start: index])\n",
    "            start = index + 1\n",
    "        index += 1\n",
    "    rack_chunks.append(s[start: index])  # Add the last line\n",
    "\n",
    "    node_names = set()\n",
    "\n",
    "    for rack_chunk in rack_chunks:\n",
    "        if \"[\" in rack_chunk:\n",
    "            prefix, postfix = rack_chunk.split(\"[\")\n",
    "            postfix = postfix[:-1]  # Remove the last bracket\n",
    "            nodes = postfix.split(\",\")\n",
    "            for node in nodes:\n",
    "                if \"-\" in node:\n",
    "                    start, end = node.split(\"-\")\n",
    "                    if not start.isnumeric() or not end.isnumeric():\n",
    "                        continue\n",
    "                    for i in range(int(start), int(end) + 1):\n",
    "                        node_names.add(\"{}{}\".format(prefix, i))\n",
    "                else:\n",
    "                    node_names.add(\"{}{}\".format(prefix, node))\n",
    "        else:\n",
    "            node_names.add(rack_chunk)\n",
    "\n",
    "    return node_names\n",
    "\n",
    "# node_names = split_nodes(\"r26n[4,7-16,29]\")\n",
    "# node_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unify data: \n",
    "# ->AveCPUFreq M/K-> M, \n",
    "# ->AveDiskRead -> M, \n",
    "# ->AveDiskWrite -> M, \n",
    "# ->MaxDiskRead -> M, \n",
    "# ->MaxDiskWrite -> M, \n",
    "# ->MaxVMSize -> M, \n",
    "# ->ReqMem M/G-> M,\n",
    "# ->NodeList -> r1n[1-3], r2n[4-5] sepereate to a list: [r1n1, r1n2, r1n3, r2n4, r2n5]\n",
    "\n",
    "def unify_ave_cpu_freq(df):\n",
    "    ave_cpu_freq_l = []\n",
    "    for i in df[\"AveCPUFreq\"]:\n",
    "        if pd.isnull(i):\n",
    "            ave_cpu_freq_num = i\n",
    "        else:\n",
    "            ave_cpu_freq = re.findall(r\"\\d+\\.?\\d*\", i)\n",
    "            ave_cpu_freq_num = ave_cpu_freq[0]\n",
    "            if 'K' in i:\n",
    "                ave_cpu_freq_num = format(float(ave_cpu_freq_num)/1024.00, '.2f')\n",
    "            else:\n",
    "                ave_cpu_freq_num = format(float(ave_cpu_freq_num)/1.00, '.2f')\n",
    "        ave_cpu_freq_l.append(ave_cpu_freq_num)\n",
    "    df[\"ave_cpu_freq\"] = ave_cpu_freq_l\n",
    "\n",
    "def unify_ave_disk_read(df):\n",
    "    ave_disk_read_l = []\n",
    "    for i in df[\"AveDiskRead\"]:\n",
    "        if pd.isnull(i):\n",
    "            ave_disk_read_num = i\n",
    "        else:\n",
    "            ave_disk_read = re.findall(r\"\\d+\\.?\\d*\", i)\n",
    "            ave_disk_read_num = ave_disk_read[0]\n",
    "            if 'G' in i:\n",
    "                ave_disk_read_num = format(float(ave_disk_read_num)*1024.00, '.2f')\n",
    "            else:\n",
    "                ave_disk_read_num = format(float(ave_disk_read_num)*1.00, '.2f')\n",
    "        ave_disk_read_l.append(ave_disk_read_num)\n",
    "    df[\"ave_disk_read\"] = ave_disk_read_l\n",
    "    \n",
    "def unify_ave_disk_write(df):\n",
    "    ave_disk_write_l = []\n",
    "    for i in df[\"AveDiskWrite\"]:\n",
    "        if pd.isnull(i):\n",
    "            ave_disk_write_num = i\n",
    "        else:\n",
    "            ave_disk_write = re.findall(r\"\\d+\\.?\\d*\", i)\n",
    "            ave_disk_write_num = ave_disk_write[0]\n",
    "            if 'G' in i:\n",
    "                ave_disk_write_num = format(float(ave_disk_write_num)*1024.00, '.2f')\n",
    "            else:\n",
    "                ave_disk_write_num = format(float(ave_disk_write_num)*1.00, '.2f')\n",
    "        ave_disk_write_l.append(ave_disk_write_num)\n",
    "    df[\"ave_disk_write\"] = ave_disk_write_l\n",
    "\n",
    "def unify_max_disk_read(df):\n",
    "    max_disk_read_l = []\n",
    "    for i in df[\"MaxDiskRead\"]:\n",
    "        if pd.isnull(i):\n",
    "            max_disk_read_num = i\n",
    "        else:\n",
    "            max_disk_read = re.findall(r\"\\d+\\.?\\d*\", i)\n",
    "            max_disk_read_num = max_disk_read[0]\n",
    "            if 'G' in i:\n",
    "                max_disk_read_num = format(float(max_disk_read_num)*1024.00, '.2f')\n",
    "            else:\n",
    "                max_disk_read_num = format(float(max_disk_read_num)*1.00, '.2f')\n",
    "        max_disk_read_l.append(max_disk_read_num)\n",
    "    df[\"max_disk_read\"] = max_disk_read_l\n",
    "\n",
    "def unify_max_disk_write(df):\n",
    "    max_disk_write_l = []\n",
    "    for i in df[\"MaxDiskWrite\"]:\n",
    "        if pd.isnull(i):\n",
    "            max_disk_write_num = i\n",
    "        else:\n",
    "            max_disk_write = re.findall(r\"\\d+\\.?\\d*\", i)\n",
    "            max_disk_write_num = max_disk_write[0]\n",
    "            if 'G' in i:\n",
    "                max_disk_write_num = format(float(max_disk_write_num)*1024.00, '.2f')\n",
    "            else:\n",
    "                max_disk_write_num = format(float(max_disk_write_num)*1.00, '.2f')\n",
    "        max_disk_write_l.append(max_disk_write_num)\n",
    "    df[\"max_disk_write\"] = max_disk_write_l\n",
    "    \n",
    "def unify_max_vm_size(df):\n",
    "    max_vm_size_l = []\n",
    "    for i in df[\"MaxDiskWrite\"]:\n",
    "        if pd.isnull(i):\n",
    "            max_vm_size_num = i\n",
    "        else:\n",
    "            max_vm_size = re.findall(r\"\\d+\\.?\\d*\", i)\n",
    "            max_vm_size_num = max_vm_size[0]\n",
    "            if 'G' in i:\n",
    "                max_vm_size_num = format(float(max_vm_size_num)*1024.00, '.2f')\n",
    "            else:\n",
    "                max_vm_size_num = format(float(max_vm_size_num)*1.00, '.2f')\n",
    "        max_vm_size_l.append(max_vm_size_num)\n",
    "    df[\"max_vm_size\"] = max_vm_size_l\n",
    "    \n",
    "def unify_req_mem(df):\n",
    "    req_mem_l = []\n",
    "    for i in df[\"ReqMem\"]:\n",
    "        if pd.isnull(i):\n",
    "            req_mem_num = i\n",
    "        else:\n",
    "            req_mem = re.findall(r\"\\d+\\.?\\d*\", i)\n",
    "            req_mem_num = req_mem[0]\n",
    "            if 'G' in i:\n",
    "                req_mem_num = format(float(req_mem_num)*1024.00, '.2f')\n",
    "            else:\n",
    "                req_mem_num = format(float(req_mem_num)*1.00, '.2f')\n",
    "        req_mem_l.append(req_mem_num)\n",
    "    df[\"req_mem\"] = req_mem_l\n",
    "\n",
    "def unify_node_list(df):\n",
    "    node_list = []\n",
    "    for node_name in df[\"NodeList\"]:\n",
    "        node_name_list = split_nodes(node_name)\n",
    "        node_list.append(node_name_list)\n",
    "    df[\"node_list\"] = node_list\n",
    "\n",
    "def unify_data(df):\n",
    "    unify_ave_cpu_freq(df)\n",
    "    unify_ave_disk_read(df)\n",
    "    unify_ave_disk_write(df)\n",
    "    unify_max_disk_read(df)\n",
    "    unify_max_disk_write(df)\n",
    "    unify_max_vm_size(df)\n",
    "    unify_req_mem(df)\n",
    "    unify_node_list(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unify_data(data)\n",
    "# data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"ExitCode\", \"AveCPUFreq\", \"AveDiskRead\", \"AveDiskWrite\", \"MaxDiskRead\", \"MaxDiskWrite\", \"MaxVMSize\", \"ReqMem\"], axis = 1, inplace = True)\n",
    "# data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal                        1372932\n",
       "shared                         581609\n",
       "gpu_shared                     104591\n",
       "gpu_titanrtx_shared             63530\n",
       "gpu_shared_course               45517\n",
       "gpu_titan+                      21019\n",
       "gpu_titanrtx                    14695\n",
       "gpu                             11950\n",
       "shared_52c_384g                 11351\n",
       "gpu_titanrtx_shared_course      11242\n",
       "fat_soil_shared                  7049\n",
       "gpu_short                        6159\n",
       "gpu_share+                       4507\n",
       "sw                               4394\n",
       "shared_jupyter                   2903\n",
       "fat                              2621\n",
       "gpu_titanrtx_short               2501\n",
       "short                            1701\n",
       "fat_soil_+                       1567\n",
       "gpu_shared_jupyter               1563\n",
       "hared_ju+                         344\n",
       "Name: Partition, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "num_partition = data[\"Partition\"].value_counts()\n",
    "display(num_partition)\n",
    "print(len(num_partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct new fields: \n",
    "#   Submit, Start, End\n",
    "#   submit_hour_of_day, submit_day_of_week, submit_date\n",
    "#   waiting_time=Start-Submit, running_time=End-Start\n",
    "\n",
    "data['Submit'] = pd.to_datetime(data['Submit'], utc=False, format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "data['Start'] = pd.to_datetime(data['Start'], utc=False, format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "data['End'] = pd.to_datetime(data['End'], utc=False, format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "data[\"submit_hour_of_day\"] = data[\"Submit\"].dt.hour\n",
    "data[\"submit_day_of_week\"] = data[\"Submit\"].apply(lambda x:x.weekday())\n",
    "data['sumbit_day_of_month'] = data['Submit'].dt.day\n",
    "data[\"submit_date\"] = data[\"Submit\"].dt.date\n",
    "\n",
    "data[\"waiting_time\"] = data[\"Start\"] - data[\"Submit\"]\n",
    "data[\"waiting_time\"] = data[\"waiting_time\"].apply(lambda x:timedelta.total_seconds(x))\n",
    "\n",
    "data[\"running_time\"] = data[\"End\"] - data[\"Start\"]\n",
    "data[\"running_time\"] = data[\"running_time\"].apply(lambda x:timedelta.total_seconds(x))\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ml jobs and tasks\n",
    "gpu_nodes = {\n",
    "    \"r28n1\", \"r28n2\", \"r28n3\", \"r28n4\", \"r28n5\",\n",
    "    \"r29n1\", \"r29n2\", \"r29n3\", \"r29n4\", \"r29n5\",\n",
    "    \"r30n1\", \"r30n2\", \"r30n3\", \"r30n4\", \"r30n5\", \"r30n6\", \"r30n7\",\n",
    "    \"r31n1\", \"r31n2\", \"r31n3\", \"r31n4\", \"r31n5\", \"r31n6\"\n",
    "    \"r32n1\", \"r32n2\", \"r32n3\", \"r32n4\", \"r32n5\", \"r32n6\", \"r32n7\",\n",
    "    \"r33n2\", \"r33n3\", \"r33n5\", \"r33n6\",\n",
    "    \"r34n1\", \"r34n2\", \"r34n3\", \"r34n4\", \"r34n5\", \"r34n6\", \"r34n7\",\n",
    "    \"r35n1\", \"r35n2\", \"r35n3\", \"r35n4\", \"r35n5\",\n",
    "    \"r36n1\", \"r36n2\", \"r36n3\", \"r36n4\", \"r36n5\",\n",
    "    \"r38n1\", \"r38n2\", \"r38n3\", \"r38n4\", \"r38n5\",\n",
    "}\n",
    "\n",
    "def mark_ml_data(df):\n",
    "    node_type_l = []\n",
    "    for i in df[\"node_list\"]:\n",
    "        if any(n in gpu_nodes for n in i):\n",
    "            node_type = 0\n",
    "        else: \n",
    "            node_type = 1\n",
    "        node_type_l.append(node_type)\n",
    "    df[\"node_type\"] = node_type_l\n",
    "\n",
    "mark_ml_data(data)\n",
    "data.drop(\"NodeList\", axis = 1, inplace = True)\n",
    "\n",
    "ml_data = data.loc[data[\"node_type\"] == 0]\n",
    "generic_data = data.loc[data[\"node_type\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract jobs and tasks run on the node \"gpu_titanrtx_shared_course\"\n",
    "def mark_course_data(df):\n",
    "    for i in df.index:\n",
    "        if \"course\" in df[\"Partition\"].iloc[i]: df[\"node_type\"].iloc[i] = 2\n",
    "        if \"education\" in df[\"Partition\"].iloc[i]: df[\"node_type\"].iloc[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/ckwc3nwj0cjbxyphnn5k3nl40000gp/T/ipykernel_21676/1754493729.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if \"course\" in df[\"Partition\"].iloc[i]: df[\"node_type\"].iloc[i] = 2\n"
     ]
    }
   ],
   "source": [
    "mark_course_data(data)\n",
    "course_data = data.loc[data[\"node_type\"] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2143897\n",
       "0     228290\n",
       "2      53537\n",
       "Name: node_type, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['node_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/ckwc3nwj0cjbxyphnn5k3nl40000gp/T/ipykernel_21676/1403585835.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ml_data.drop(columns = \"node_type\", inplace = True)\n",
      "/var/folders/_0/ckwc3nwj0cjbxyphnn5k3nl40000gp/T/ipykernel_21676/1403585835.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  generic_data.drop(columns = \"node_type\", inplace = True)\n",
      "/var/folders/_0/ckwc3nwj0cjbxyphnn5k3nl40000gp/T/ipykernel_21676/1403585835.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  course_data.drop(columns = \"node_type\", inplace = True)\n"
     ]
    }
   ],
   "source": [
    "ml_data.drop(columns = \"node_type\", inplace = True)\n",
    "generic_data.drop(columns = \"node_type\", inplace = True)\n",
    "course_data.drop(columns = \"node_type\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract jobs and tasks run on the node \"gpu_titanrtx_shared_course\"\n",
    "# course_data = ml_data.loc[ml_data['Partition'].str.contains(\"course|education\", na = False)]\n",
    "# course_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned ml data and generic data\n",
    "file_path_2 = \"/Users/yzang/Documents/Slurm data/Clean data/\"\n",
    "\n",
    "data.to_csv(file_path_2 + \"slurm_data_cleaned.csv\")\n",
    "ml_data.to_csv(file_path_2 + \"ml_data_cleaned.csv\")\n",
    "course_data.to_csv(file_path_2 + \"course_data_cleaned.csv\")\n",
    "generic_data.to_csv(file_path_2 + \"generic_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JobID                   0.00\n",
       "GID                     6.27\n",
       "UID                     6.27\n",
       "Partition               6.27\n",
       "Submit                  0.00\n",
       "Start                   0.00\n",
       "End                     0.00\n",
       "ElapsedRaw              0.00\n",
       "CPUTimeRAW              0.00\n",
       "NCPUS                   0.00\n",
       "NNode                   0.00\n",
       "State                   0.00\n",
       "AllocCPUS               0.00\n",
       "AllocNode               0.00\n",
       "NTask                  93.73\n",
       "ReqCPUS                 0.00\n",
       "ave_cpu_freq           93.73\n",
       "ave_disk_read          93.79\n",
       "ave_disk_write         93.79\n",
       "max_disk_read          93.79\n",
       "max_disk_write         93.79\n",
       "max_vm_size            93.79\n",
       "req_mem                 6.27\n",
       "node_list               0.00\n",
       "submit_hour_of_day      0.00\n",
       "submit_day_of_week      0.00\n",
       "sumbit_day_of_month     0.00\n",
       "submit_date             0.00\n",
       "waiting_time            0.00\n",
       "running_time            0.00\n",
       "node_type               0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.isna().sum()\n",
    "missing_per = round(data.isnull().mean() * 100, 2)\n",
    "missing_per"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
